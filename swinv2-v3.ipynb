{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## wd-swinv2-tagger-v3\n",
    "\n",
    "References:\n",
    "- Original model: https://huggingface.co/SmilingWolf/wd-swinv2-tagger-v3\n",
    "- Old training repo: https://github.com/SmilingWolf/SW-CV-ModelZoo\n",
    "- v3 repo: https://github.com/SmilingWolf/JAX-CV\n",
    "- Converting script: https://github.com/huggingface/transformers/blob/main/src/transformers/models/swinv2/convert_swinv2_timm_to_pytorch.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clone the swinv2 model\n",
    "!git clone https://huggingface.co/SmilingWolf/wd-swinv2-tagger-v3 ./swinv2-v3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "\n",
    "from transformers import AutoImageProcessor, Swinv2Config, Swinv2ForImageClassification\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Swinv2Config.from_pretrained(\"swinv2-v3-hf\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./swinv2-v3/selected_tags.csv\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tag_name(tag: str, category: int):\n",
    "    if category == 0:\n",
    "        return tag\n",
    "    elif category == 4:\n",
    "        return f\"character:{tag}\"\n",
    "    elif category == 9:\n",
    "        return f\"rating:{tag}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    i: convert_tag_name(tag, df[\"category\"][i]) for i, tag in enumerate(df[\"name\"])\n",
    "}\n",
    "id2label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.id2label = id2label\n",
    "config.label2id = {v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.save_pretrained(\"swinv2-v3-hf\")  # save the config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load models and convert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_key(name):\n",
    "    if \"patch_embed.proj\" in name:\n",
    "        name = name.replace(\n",
    "            \"patch_embed.proj\", \"embeddings.patch_embeddings.projection\"\n",
    "        )\n",
    "    if \"patch_embed.norm\" in name:\n",
    "        name = name.replace(\"patch_embed.norm\", \"embeddings.norm\")\n",
    "    if \"layers\" in name:\n",
    "        name = \"encoder.\" + name\n",
    "    if \"attn.proj\" in name:\n",
    "        name = name.replace(\"attn.proj\", \"attention.output.dense\")\n",
    "    if \"attn\" in name:\n",
    "        name = name.replace(\"attn\", \"attention.self\")\n",
    "    if \"norm1\" in name:\n",
    "        name = name.replace(\"norm1\", \"layernorm_before\")\n",
    "    if \"norm2\" in name:\n",
    "        name = name.replace(\"norm2\", \"layernorm_after\")\n",
    "    if \"mlp.fc1\" in name:\n",
    "        name = name.replace(\"mlp.fc1\", \"intermediate.dense\")\n",
    "    if \"mlp.fc2\" in name:\n",
    "        name = name.replace(\"mlp.fc2\", \"output.dense\")\n",
    "    if \"q_bias\" in name:\n",
    "        name = name.replace(\"q_bias\", \"query.bias\")\n",
    "    if \"k_bias\" in name:\n",
    "        name = name.replace(\"k_bias\", \"key.bias\")\n",
    "    if \"v_bias\" in name:\n",
    "        name = name.replace(\"v_bias\", \"value.bias\")\n",
    "    if \"cpb_mlp\" in name:\n",
    "        name = name.replace(\"cpb_mlp\", \"continuous_position_bias_mlp\")\n",
    "    if name == \"norm.weight\":\n",
    "        name = \"layernorm.weight\"\n",
    "    if name == \"norm.bias\":\n",
    "        name = \"layernorm.bias\"\n",
    "\n",
    "    if \"head.fc\" in name:\n",
    "        name = name.replace(\"head.fc\", \"classifier\")\n",
    "    else:\n",
    "        name = \"swinv2.\" + name\n",
    "\n",
    "    if \"1.downsample\" in name:\n",
    "        name = name.replace(\"1.downsample\", \"0.downsample\")\n",
    "    elif \"2.downsample\" in name:\n",
    "        name = name.replace(\"2.downsample\", \"1.downsample\")\n",
    "    elif \"3.downsample\" in name:\n",
    "        name = name.replace(\"3.downsample\", \"2.downsample\")\n",
    "\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_state_dict(orig_state_dict, model):\n",
    "    for key in orig_state_dict.copy().keys():\n",
    "        val = orig_state_dict.pop(key)\n",
    "\n",
    "        if \"mask\" in key:\n",
    "            continue\n",
    "        elif \"qkv\" in key:\n",
    "            key_split = key.split(\".\")\n",
    "            layer_num = int(key_split[1])\n",
    "            block_num = int(key_split[3])\n",
    "            dim = (\n",
    "                model.swinv2.encoder.layers[layer_num]\n",
    "                .blocks[block_num]\n",
    "                .attention.self.all_head_size\n",
    "            )\n",
    "\n",
    "            if \"weight\" in key:\n",
    "                orig_state_dict[\n",
    "                    f\"swinv2.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.query.weight\"\n",
    "                ] = val[:dim, :]\n",
    "                orig_state_dict[\n",
    "                    f\"swinv2.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.key.weight\"\n",
    "                ] = val[dim : dim * 2, :]\n",
    "                orig_state_dict[\n",
    "                    f\"swinv2.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.value.weight\"\n",
    "                ] = val[-dim:, :]\n",
    "            else:\n",
    "                orig_state_dict[\n",
    "                    f\"swinv2.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.query.bias\"\n",
    "                ] = val[:dim]\n",
    "                orig_state_dict[\n",
    "                    f\"swinv2.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.key.bias\"\n",
    "                ] = val[dim : dim * 2]\n",
    "                orig_state_dict[\n",
    "                    f\"swinv2.encoder.layers.{layer_num}.blocks.{block_num}.attention.self.value.bias\"\n",
    "                ] = val[-dim:]\n",
    "        else:\n",
    "            orig_state_dict[rename_key(key)] = val\n",
    "\n",
    "    return orig_state_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "timm_file = load_file(\"./swinv2-v3/model.safetensors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['head.fc.bias',\n",
       " 'head.fc.weight',\n",
       " 'layers.0.blocks.0.attn.cpb_mlp.0.bias',\n",
       " 'layers.0.blocks.0.attn.cpb_mlp.0.weight',\n",
       " 'layers.0.blocks.0.attn.cpb_mlp.2.weight',\n",
       " 'layers.0.blocks.0.attn.logit_scale',\n",
       " 'layers.0.blocks.0.attn.proj.bias',\n",
       " 'layers.0.blocks.0.attn.proj.weight',\n",
       " 'layers.0.blocks.0.attn.q_bias',\n",
       " 'layers.0.blocks.0.attn.qkv.weight',\n",
       " 'layers.0.blocks.0.attn.v_bias',\n",
       " 'layers.0.blocks.0.mlp.fc1.bias',\n",
       " 'layers.0.blocks.0.mlp.fc1.weight',\n",
       " 'layers.0.blocks.0.mlp.fc2.bias',\n",
       " 'layers.0.blocks.0.mlp.fc2.weight',\n",
       " 'layers.0.blocks.0.norm1.bias',\n",
       " 'layers.0.blocks.0.norm1.weight',\n",
       " 'layers.0.blocks.0.norm2.bias',\n",
       " 'layers.0.blocks.0.norm2.weight',\n",
       " 'layers.0.blocks.1.attn.cpb_mlp.0.bias',\n",
       " 'layers.0.blocks.1.attn.cpb_mlp.0.weight',\n",
       " 'layers.0.blocks.1.attn.cpb_mlp.2.weight',\n",
       " 'layers.0.blocks.1.attn.logit_scale',\n",
       " 'layers.0.blocks.1.attn.proj.bias',\n",
       " 'layers.0.blocks.1.attn.proj.weight',\n",
       " 'layers.0.blocks.1.attn.q_bias',\n",
       " 'layers.0.blocks.1.attn.qkv.weight',\n",
       " 'layers.0.blocks.1.attn.v_bias',\n",
       " 'layers.0.blocks.1.mlp.fc1.bias',\n",
       " 'layers.0.blocks.1.mlp.fc1.weight',\n",
       " 'layers.0.blocks.1.mlp.fc2.bias',\n",
       " 'layers.0.blocks.1.mlp.fc2.weight',\n",
       " 'layers.0.blocks.1.norm1.bias',\n",
       " 'layers.0.blocks.1.norm1.weight',\n",
       " 'layers.0.blocks.1.norm2.bias',\n",
       " 'layers.0.blocks.1.norm2.weight',\n",
       " 'layers.1.blocks.0.attn.cpb_mlp.0.bias',\n",
       " 'layers.1.blocks.0.attn.cpb_mlp.0.weight',\n",
       " 'layers.1.blocks.0.attn.cpb_mlp.2.weight',\n",
       " 'layers.1.blocks.0.attn.logit_scale',\n",
       " 'layers.1.blocks.0.attn.proj.bias',\n",
       " 'layers.1.blocks.0.attn.proj.weight',\n",
       " 'layers.1.blocks.0.attn.q_bias',\n",
       " 'layers.1.blocks.0.attn.qkv.weight',\n",
       " 'layers.1.blocks.0.attn.v_bias',\n",
       " 'layers.1.blocks.0.mlp.fc1.bias',\n",
       " 'layers.1.blocks.0.mlp.fc1.weight',\n",
       " 'layers.1.blocks.0.mlp.fc2.bias',\n",
       " 'layers.1.blocks.0.mlp.fc2.weight',\n",
       " 'layers.1.blocks.0.norm1.bias',\n",
       " 'layers.1.blocks.0.norm1.weight',\n",
       " 'layers.1.blocks.0.norm2.bias',\n",
       " 'layers.1.blocks.0.norm2.weight',\n",
       " 'layers.1.blocks.1.attn.cpb_mlp.0.bias',\n",
       " 'layers.1.blocks.1.attn.cpb_mlp.0.weight',\n",
       " 'layers.1.blocks.1.attn.cpb_mlp.2.weight',\n",
       " 'layers.1.blocks.1.attn.logit_scale',\n",
       " 'layers.1.blocks.1.attn.proj.bias',\n",
       " 'layers.1.blocks.1.attn.proj.weight',\n",
       " 'layers.1.blocks.1.attn.q_bias',\n",
       " 'layers.1.blocks.1.attn.qkv.weight',\n",
       " 'layers.1.blocks.1.attn.v_bias',\n",
       " 'layers.1.blocks.1.mlp.fc1.bias',\n",
       " 'layers.1.blocks.1.mlp.fc1.weight',\n",
       " 'layers.1.blocks.1.mlp.fc2.bias',\n",
       " 'layers.1.blocks.1.mlp.fc2.weight',\n",
       " 'layers.1.blocks.1.norm1.bias',\n",
       " 'layers.1.blocks.1.norm1.weight',\n",
       " 'layers.1.blocks.1.norm2.bias',\n",
       " 'layers.1.blocks.1.norm2.weight',\n",
       " 'layers.1.downsample.norm.bias',\n",
       " 'layers.1.downsample.norm.weight',\n",
       " 'layers.1.downsample.reduction.weight',\n",
       " 'layers.2.blocks.0.attn.cpb_mlp.0.bias',\n",
       " 'layers.2.blocks.0.attn.cpb_mlp.0.weight',\n",
       " 'layers.2.blocks.0.attn.cpb_mlp.2.weight',\n",
       " 'layers.2.blocks.0.attn.logit_scale',\n",
       " 'layers.2.blocks.0.attn.proj.bias',\n",
       " 'layers.2.blocks.0.attn.proj.weight',\n",
       " 'layers.2.blocks.0.attn.q_bias',\n",
       " 'layers.2.blocks.0.attn.qkv.weight',\n",
       " 'layers.2.blocks.0.attn.v_bias',\n",
       " 'layers.2.blocks.0.mlp.fc1.bias',\n",
       " 'layers.2.blocks.0.mlp.fc1.weight',\n",
       " 'layers.2.blocks.0.mlp.fc2.bias',\n",
       " 'layers.2.blocks.0.mlp.fc2.weight',\n",
       " 'layers.2.blocks.0.norm1.bias',\n",
       " 'layers.2.blocks.0.norm1.weight',\n",
       " 'layers.2.blocks.0.norm2.bias',\n",
       " 'layers.2.blocks.0.norm2.weight',\n",
       " 'layers.2.blocks.1.attn.cpb_mlp.0.bias',\n",
       " 'layers.2.blocks.1.attn.cpb_mlp.0.weight',\n",
       " 'layers.2.blocks.1.attn.cpb_mlp.2.weight',\n",
       " 'layers.2.blocks.1.attn.logit_scale',\n",
       " 'layers.2.blocks.1.attn.proj.bias',\n",
       " 'layers.2.blocks.1.attn.proj.weight',\n",
       " 'layers.2.blocks.1.attn.q_bias',\n",
       " 'layers.2.blocks.1.attn.qkv.weight',\n",
       " 'layers.2.blocks.1.attn.v_bias',\n",
       " 'layers.2.blocks.1.mlp.fc1.bias',\n",
       " 'layers.2.blocks.1.mlp.fc1.weight',\n",
       " 'layers.2.blocks.1.mlp.fc2.bias',\n",
       " 'layers.2.blocks.1.mlp.fc2.weight',\n",
       " 'layers.2.blocks.1.norm1.bias',\n",
       " 'layers.2.blocks.1.norm1.weight',\n",
       " 'layers.2.blocks.1.norm2.bias',\n",
       " 'layers.2.blocks.1.norm2.weight',\n",
       " 'layers.2.blocks.10.attn.cpb_mlp.0.bias',\n",
       " 'layers.2.blocks.10.attn.cpb_mlp.0.weight',\n",
       " 'layers.2.blocks.10.attn.cpb_mlp.2.weight',\n",
       " 'layers.2.blocks.10.attn.logit_scale',\n",
       " 'layers.2.blocks.10.attn.proj.bias',\n",
       " 'layers.2.blocks.10.attn.proj.weight',\n",
       " 'layers.2.blocks.10.attn.q_bias',\n",
       " 'layers.2.blocks.10.attn.qkv.weight',\n",
       " 'layers.2.blocks.10.attn.v_bias',\n",
       " 'layers.2.blocks.10.mlp.fc1.bias',\n",
       " 'layers.2.blocks.10.mlp.fc1.weight',\n",
       " 'layers.2.blocks.10.mlp.fc2.bias',\n",
       " 'layers.2.blocks.10.mlp.fc2.weight',\n",
       " 'layers.2.blocks.10.norm1.bias',\n",
       " 'layers.2.blocks.10.norm1.weight',\n",
       " 'layers.2.blocks.10.norm2.bias',\n",
       " 'layers.2.blocks.10.norm2.weight',\n",
       " 'layers.2.blocks.11.attn.cpb_mlp.0.bias',\n",
       " 'layers.2.blocks.11.attn.cpb_mlp.0.weight',\n",
       " 'layers.2.blocks.11.attn.cpb_mlp.2.weight',\n",
       " 'layers.2.blocks.11.attn.logit_scale',\n",
       " 'layers.2.blocks.11.attn.proj.bias',\n",
       " 'layers.2.blocks.11.attn.proj.weight',\n",
       " 'layers.2.blocks.11.attn.q_bias',\n",
       " 'layers.2.blocks.11.attn.qkv.weight',\n",
       " 'layers.2.blocks.11.attn.v_bias',\n",
       " 'layers.2.blocks.11.mlp.fc1.bias',\n",
       " 'layers.2.blocks.11.mlp.fc1.weight',\n",
       " 'layers.2.blocks.11.mlp.fc2.bias',\n",
       " 'layers.2.blocks.11.mlp.fc2.weight',\n",
       " 'layers.2.blocks.11.norm1.bias',\n",
       " 'layers.2.blocks.11.norm1.weight',\n",
       " 'layers.2.blocks.11.norm2.bias',\n",
       " 'layers.2.blocks.11.norm2.weight',\n",
       " 'layers.2.blocks.12.attn.cpb_mlp.0.bias',\n",
       " 'layers.2.blocks.12.attn.cpb_mlp.0.weight',\n",
       " 'layers.2.blocks.12.attn.cpb_mlp.2.weight',\n",
       " 'layers.2.blocks.12.attn.logit_scale',\n",
       " 'layers.2.blocks.12.attn.proj.bias',\n",
       " 'layers.2.blocks.12.attn.proj.weight',\n",
       " 'layers.2.blocks.12.attn.q_bias',\n",
       " 'layers.2.blocks.12.attn.qkv.weight',\n",
       " 'layers.2.blocks.12.attn.v_bias',\n",
       " 'layers.2.blocks.12.mlp.fc1.bias',\n",
       " 'layers.2.blocks.12.mlp.fc1.weight',\n",
       " 'layers.2.blocks.12.mlp.fc2.bias',\n",
       " 'layers.2.blocks.12.mlp.fc2.weight',\n",
       " 'layers.2.blocks.12.norm1.bias',\n",
       " 'layers.2.blocks.12.norm1.weight',\n",
       " 'layers.2.blocks.12.norm2.bias',\n",
       " 'layers.2.blocks.12.norm2.weight',\n",
       " 'layers.2.blocks.13.attn.cpb_mlp.0.bias',\n",
       " 'layers.2.blocks.13.attn.cpb_mlp.0.weight',\n",
       " 'layers.2.blocks.13.attn.cpb_mlp.2.weight',\n",
       " 'layers.2.blocks.13.attn.logit_scale',\n",
       " 'layers.2.blocks.13.attn.proj.bias',\n",
       " 'layers.2.blocks.13.attn.proj.weight',\n",
       " 'layers.2.blocks.13.attn.q_bias',\n",
       " 'layers.2.blocks.13.attn.qkv.weight',\n",
       " 'layers.2.blocks.13.attn.v_bias',\n",
       " 'layers.2.blocks.13.mlp.fc1.bias',\n",
       " 'layers.2.blocks.13.mlp.fc1.weight',\n",
       " 'layers.2.blocks.13.mlp.fc2.bias',\n",
       " 'layers.2.blocks.13.mlp.fc2.weight',\n",
       " 'layers.2.blocks.13.norm1.bias',\n",
       " 'layers.2.blocks.13.norm1.weight',\n",
       " 'layers.2.blocks.13.norm2.bias',\n",
       " 'layers.2.blocks.13.norm2.weight',\n",
       " 'layers.2.blocks.14.attn.cpb_mlp.0.bias',\n",
       " 'layers.2.blocks.14.attn.cpb_mlp.0.weight',\n",
       " 'layers.2.blocks.14.attn.cpb_mlp.2.weight',\n",
       " 'layers.2.blocks.14.attn.logit_scale',\n",
       " 'layers.2.blocks.14.attn.proj.bias',\n",
       " 'layers.2.blocks.14.attn.proj.weight',\n",
       " 'layers.2.blocks.14.attn.q_bias',\n",
       " 'layers.2.blocks.14.attn.qkv.weight',\n",
       " 'layers.2.blocks.14.attn.v_bias',\n",
       " 'layers.2.blocks.14.mlp.fc1.bias',\n",
       " 'layers.2.blocks.14.mlp.fc1.weight',\n",
       " 'layers.2.blocks.14.mlp.fc2.bias',\n",
       " 'layers.2.blocks.14.mlp.fc2.weight',\n",
       " 'layers.2.blocks.14.norm1.bias',\n",
       " 'layers.2.blocks.14.norm1.weight',\n",
       " 'layers.2.blocks.14.norm2.bias',\n",
       " 'layers.2.blocks.14.norm2.weight',\n",
       " 'layers.2.blocks.15.attn.cpb_mlp.0.bias',\n",
       " 'layers.2.blocks.15.attn.cpb_mlp.0.weight',\n",
       " 'layers.2.blocks.15.attn.cpb_mlp.2.weight',\n",
       " 'layers.2.blocks.15.attn.logit_scale',\n",
       " 'layers.2.blocks.15.attn.proj.bias',\n",
       " 'layers.2.blocks.15.attn.proj.weight',\n",
       " 'layers.2.blocks.15.attn.q_bias',\n",
       " 'layers.2.blocks.15.attn.qkv.weight',\n",
       " 'layers.2.blocks.15.attn.v_bias',\n",
       " 'layers.2.blocks.15.mlp.fc1.bias',\n",
       " 'layers.2.blocks.15.mlp.fc1.weight',\n",
       " 'layers.2.blocks.15.mlp.fc2.bias',\n",
       " 'layers.2.blocks.15.mlp.fc2.weight',\n",
       " 'layers.2.blocks.15.norm1.bias',\n",
       " 'layers.2.blocks.15.norm1.weight',\n",
       " 'layers.2.blocks.15.norm2.bias',\n",
       " 'layers.2.blocks.15.norm2.weight',\n",
       " 'layers.2.blocks.16.attn.cpb_mlp.0.bias',\n",
       " 'layers.2.blocks.16.attn.cpb_mlp.0.weight',\n",
       " 'layers.2.blocks.16.attn.cpb_mlp.2.weight',\n",
       " 'layers.2.blocks.16.attn.logit_scale',\n",
       " 'layers.2.blocks.16.attn.proj.bias',\n",
       " 'layers.2.blocks.16.attn.proj.weight',\n",
       " 'layers.2.blocks.16.attn.q_bias',\n",
       " 'layers.2.blocks.16.attn.qkv.weight',\n",
       " 'layers.2.blocks.16.attn.v_bias',\n",
       " 'layers.2.blocks.16.mlp.fc1.bias',\n",
       " 'layers.2.blocks.16.mlp.fc1.weight',\n",
       " 'layers.2.blocks.16.mlp.fc2.bias',\n",
       " 'layers.2.blocks.16.mlp.fc2.weight',\n",
       " 'layers.2.blocks.16.norm1.bias',\n",
       " 'layers.2.blocks.16.norm1.weight',\n",
       " 'layers.2.blocks.16.norm2.bias',\n",
       " 'layers.2.blocks.16.norm2.weight',\n",
       " 'layers.2.blocks.17.attn.cpb_mlp.0.bias',\n",
       " 'layers.2.blocks.17.attn.cpb_mlp.0.weight',\n",
       " 'layers.2.blocks.17.attn.cpb_mlp.2.weight',\n",
       " 'layers.2.blocks.17.attn.logit_scale',\n",
       " 'layers.2.blocks.17.attn.proj.bias',\n",
       " 'layers.2.blocks.17.attn.proj.weight',\n",
       " 'layers.2.blocks.17.attn.q_bias',\n",
       " 'layers.2.blocks.17.attn.qkv.weight',\n",
       " 'layers.2.blocks.17.attn.v_bias',\n",
       " 'layers.2.blocks.17.mlp.fc1.bias',\n",
       " 'layers.2.blocks.17.mlp.fc1.weight',\n",
       " 'layers.2.blocks.17.mlp.fc2.bias',\n",
       " 'layers.2.blocks.17.mlp.fc2.weight',\n",
       " 'layers.2.blocks.17.norm1.bias',\n",
       " 'layers.2.blocks.17.norm1.weight',\n",
       " 'layers.2.blocks.17.norm2.bias',\n",
       " 'layers.2.blocks.17.norm2.weight',\n",
       " 'layers.2.blocks.2.attn.cpb_mlp.0.bias',\n",
       " 'layers.2.blocks.2.attn.cpb_mlp.0.weight',\n",
       " 'layers.2.blocks.2.attn.cpb_mlp.2.weight',\n",
       " 'layers.2.blocks.2.attn.logit_scale',\n",
       " 'layers.2.blocks.2.attn.proj.bias',\n",
       " 'layers.2.blocks.2.attn.proj.weight',\n",
       " 'layers.2.blocks.2.attn.q_bias',\n",
       " 'layers.2.blocks.2.attn.qkv.weight',\n",
       " 'layers.2.blocks.2.attn.v_bias',\n",
       " 'layers.2.blocks.2.mlp.fc1.bias',\n",
       " 'layers.2.blocks.2.mlp.fc1.weight',\n",
       " 'layers.2.blocks.2.mlp.fc2.bias',\n",
       " 'layers.2.blocks.2.mlp.fc2.weight',\n",
       " 'layers.2.blocks.2.norm1.bias',\n",
       " 'layers.2.blocks.2.norm1.weight',\n",
       " 'layers.2.blocks.2.norm2.bias',\n",
       " 'layers.2.blocks.2.norm2.weight',\n",
       " 'layers.2.blocks.3.attn.cpb_mlp.0.bias',\n",
       " 'layers.2.blocks.3.attn.cpb_mlp.0.weight',\n",
       " 'layers.2.blocks.3.attn.cpb_mlp.2.weight',\n",
       " 'layers.2.blocks.3.attn.logit_scale',\n",
       " 'layers.2.blocks.3.attn.proj.bias',\n",
       " 'layers.2.blocks.3.attn.proj.weight',\n",
       " 'layers.2.blocks.3.attn.q_bias',\n",
       " 'layers.2.blocks.3.attn.qkv.weight',\n",
       " 'layers.2.blocks.3.attn.v_bias',\n",
       " 'layers.2.blocks.3.mlp.fc1.bias',\n",
       " 'layers.2.blocks.3.mlp.fc1.weight',\n",
       " 'layers.2.blocks.3.mlp.fc2.bias',\n",
       " 'layers.2.blocks.3.mlp.fc2.weight',\n",
       " 'layers.2.blocks.3.norm1.bias',\n",
       " 'layers.2.blocks.3.norm1.weight',\n",
       " 'layers.2.blocks.3.norm2.bias',\n",
       " 'layers.2.blocks.3.norm2.weight',\n",
       " 'layers.2.blocks.4.attn.cpb_mlp.0.bias',\n",
       " 'layers.2.blocks.4.attn.cpb_mlp.0.weight',\n",
       " 'layers.2.blocks.4.attn.cpb_mlp.2.weight',\n",
       " 'layers.2.blocks.4.attn.logit_scale',\n",
       " 'layers.2.blocks.4.attn.proj.bias',\n",
       " 'layers.2.blocks.4.attn.proj.weight',\n",
       " 'layers.2.blocks.4.attn.q_bias',\n",
       " 'layers.2.blocks.4.attn.qkv.weight',\n",
       " 'layers.2.blocks.4.attn.v_bias',\n",
       " 'layers.2.blocks.4.mlp.fc1.bias',\n",
       " 'layers.2.blocks.4.mlp.fc1.weight',\n",
       " 'layers.2.blocks.4.mlp.fc2.bias',\n",
       " 'layers.2.blocks.4.mlp.fc2.weight',\n",
       " 'layers.2.blocks.4.norm1.bias',\n",
       " 'layers.2.blocks.4.norm1.weight',\n",
       " 'layers.2.blocks.4.norm2.bias',\n",
       " 'layers.2.blocks.4.norm2.weight',\n",
       " 'layers.2.blocks.5.attn.cpb_mlp.0.bias',\n",
       " 'layers.2.blocks.5.attn.cpb_mlp.0.weight',\n",
       " 'layers.2.blocks.5.attn.cpb_mlp.2.weight',\n",
       " 'layers.2.blocks.5.attn.logit_scale',\n",
       " 'layers.2.blocks.5.attn.proj.bias',\n",
       " 'layers.2.blocks.5.attn.proj.weight',\n",
       " 'layers.2.blocks.5.attn.q_bias',\n",
       " 'layers.2.blocks.5.attn.qkv.weight',\n",
       " 'layers.2.blocks.5.attn.v_bias',\n",
       " 'layers.2.blocks.5.mlp.fc1.bias',\n",
       " 'layers.2.blocks.5.mlp.fc1.weight',\n",
       " 'layers.2.blocks.5.mlp.fc2.bias',\n",
       " 'layers.2.blocks.5.mlp.fc2.weight',\n",
       " 'layers.2.blocks.5.norm1.bias',\n",
       " 'layers.2.blocks.5.norm1.weight',\n",
       " 'layers.2.blocks.5.norm2.bias',\n",
       " 'layers.2.blocks.5.norm2.weight',\n",
       " 'layers.2.blocks.6.attn.cpb_mlp.0.bias',\n",
       " 'layers.2.blocks.6.attn.cpb_mlp.0.weight',\n",
       " 'layers.2.blocks.6.attn.cpb_mlp.2.weight',\n",
       " 'layers.2.blocks.6.attn.logit_scale',\n",
       " 'layers.2.blocks.6.attn.proj.bias',\n",
       " 'layers.2.blocks.6.attn.proj.weight',\n",
       " 'layers.2.blocks.6.attn.q_bias',\n",
       " 'layers.2.blocks.6.attn.qkv.weight',\n",
       " 'layers.2.blocks.6.attn.v_bias',\n",
       " 'layers.2.blocks.6.mlp.fc1.bias',\n",
       " 'layers.2.blocks.6.mlp.fc1.weight',\n",
       " 'layers.2.blocks.6.mlp.fc2.bias',\n",
       " 'layers.2.blocks.6.mlp.fc2.weight',\n",
       " 'layers.2.blocks.6.norm1.bias',\n",
       " 'layers.2.blocks.6.norm1.weight',\n",
       " 'layers.2.blocks.6.norm2.bias',\n",
       " 'layers.2.blocks.6.norm2.weight',\n",
       " 'layers.2.blocks.7.attn.cpb_mlp.0.bias',\n",
       " 'layers.2.blocks.7.attn.cpb_mlp.0.weight',\n",
       " 'layers.2.blocks.7.attn.cpb_mlp.2.weight',\n",
       " 'layers.2.blocks.7.attn.logit_scale',\n",
       " 'layers.2.blocks.7.attn.proj.bias',\n",
       " 'layers.2.blocks.7.attn.proj.weight',\n",
       " 'layers.2.blocks.7.attn.q_bias',\n",
       " 'layers.2.blocks.7.attn.qkv.weight',\n",
       " 'layers.2.blocks.7.attn.v_bias',\n",
       " 'layers.2.blocks.7.mlp.fc1.bias',\n",
       " 'layers.2.blocks.7.mlp.fc1.weight',\n",
       " 'layers.2.blocks.7.mlp.fc2.bias',\n",
       " 'layers.2.blocks.7.mlp.fc2.weight',\n",
       " 'layers.2.blocks.7.norm1.bias',\n",
       " 'layers.2.blocks.7.norm1.weight',\n",
       " 'layers.2.blocks.7.norm2.bias',\n",
       " 'layers.2.blocks.7.norm2.weight',\n",
       " 'layers.2.blocks.8.attn.cpb_mlp.0.bias',\n",
       " 'layers.2.blocks.8.attn.cpb_mlp.0.weight',\n",
       " 'layers.2.blocks.8.attn.cpb_mlp.2.weight',\n",
       " 'layers.2.blocks.8.attn.logit_scale',\n",
       " 'layers.2.blocks.8.attn.proj.bias',\n",
       " 'layers.2.blocks.8.attn.proj.weight',\n",
       " 'layers.2.blocks.8.attn.q_bias',\n",
       " 'layers.2.blocks.8.attn.qkv.weight',\n",
       " 'layers.2.blocks.8.attn.v_bias',\n",
       " 'layers.2.blocks.8.mlp.fc1.bias',\n",
       " 'layers.2.blocks.8.mlp.fc1.weight',\n",
       " 'layers.2.blocks.8.mlp.fc2.bias',\n",
       " 'layers.2.blocks.8.mlp.fc2.weight',\n",
       " 'layers.2.blocks.8.norm1.bias',\n",
       " 'layers.2.blocks.8.norm1.weight',\n",
       " 'layers.2.blocks.8.norm2.bias',\n",
       " 'layers.2.blocks.8.norm2.weight',\n",
       " 'layers.2.blocks.9.attn.cpb_mlp.0.bias',\n",
       " 'layers.2.blocks.9.attn.cpb_mlp.0.weight',\n",
       " 'layers.2.blocks.9.attn.cpb_mlp.2.weight',\n",
       " 'layers.2.blocks.9.attn.logit_scale',\n",
       " 'layers.2.blocks.9.attn.proj.bias',\n",
       " 'layers.2.blocks.9.attn.proj.weight',\n",
       " 'layers.2.blocks.9.attn.q_bias',\n",
       " 'layers.2.blocks.9.attn.qkv.weight',\n",
       " 'layers.2.blocks.9.attn.v_bias',\n",
       " 'layers.2.blocks.9.mlp.fc1.bias',\n",
       " 'layers.2.blocks.9.mlp.fc1.weight',\n",
       " 'layers.2.blocks.9.mlp.fc2.bias',\n",
       " 'layers.2.blocks.9.mlp.fc2.weight',\n",
       " 'layers.2.blocks.9.norm1.bias',\n",
       " 'layers.2.blocks.9.norm1.weight',\n",
       " 'layers.2.blocks.9.norm2.bias',\n",
       " 'layers.2.blocks.9.norm2.weight',\n",
       " 'layers.2.downsample.norm.bias',\n",
       " 'layers.2.downsample.norm.weight',\n",
       " 'layers.2.downsample.reduction.weight',\n",
       " 'layers.3.blocks.0.attn.cpb_mlp.0.bias',\n",
       " 'layers.3.blocks.0.attn.cpb_mlp.0.weight',\n",
       " 'layers.3.blocks.0.attn.cpb_mlp.2.weight',\n",
       " 'layers.3.blocks.0.attn.logit_scale',\n",
       " 'layers.3.blocks.0.attn.proj.bias',\n",
       " 'layers.3.blocks.0.attn.proj.weight',\n",
       " 'layers.3.blocks.0.attn.q_bias',\n",
       " 'layers.3.blocks.0.attn.qkv.weight',\n",
       " 'layers.3.blocks.0.attn.v_bias',\n",
       " 'layers.3.blocks.0.mlp.fc1.bias',\n",
       " 'layers.3.blocks.0.mlp.fc1.weight',\n",
       " 'layers.3.blocks.0.mlp.fc2.bias',\n",
       " 'layers.3.blocks.0.mlp.fc2.weight',\n",
       " 'layers.3.blocks.0.norm1.bias',\n",
       " 'layers.3.blocks.0.norm1.weight',\n",
       " 'layers.3.blocks.0.norm2.bias',\n",
       " 'layers.3.blocks.0.norm2.weight',\n",
       " 'layers.3.blocks.1.attn.cpb_mlp.0.bias',\n",
       " 'layers.3.blocks.1.attn.cpb_mlp.0.weight',\n",
       " 'layers.3.blocks.1.attn.cpb_mlp.2.weight',\n",
       " 'layers.3.blocks.1.attn.logit_scale',\n",
       " 'layers.3.blocks.1.attn.proj.bias',\n",
       " 'layers.3.blocks.1.attn.proj.weight',\n",
       " 'layers.3.blocks.1.attn.q_bias',\n",
       " 'layers.3.blocks.1.attn.qkv.weight',\n",
       " 'layers.3.blocks.1.attn.v_bias',\n",
       " 'layers.3.blocks.1.mlp.fc1.bias',\n",
       " 'layers.3.blocks.1.mlp.fc1.weight',\n",
       " 'layers.3.blocks.1.mlp.fc2.bias',\n",
       " 'layers.3.blocks.1.mlp.fc2.weight',\n",
       " 'layers.3.blocks.1.norm1.bias',\n",
       " 'layers.3.blocks.1.norm1.weight',\n",
       " 'layers.3.blocks.1.norm2.bias',\n",
       " 'layers.3.blocks.1.norm2.weight',\n",
       " 'layers.3.downsample.norm.bias',\n",
       " 'layers.3.downsample.norm.weight',\n",
       " 'layers.3.downsample.reduction.weight',\n",
       " 'norm.bias',\n",
       " 'norm.weight',\n",
       " 'patch_embed.norm.bias',\n",
       " 'patch_embed.norm.weight',\n",
       " 'patch_embed.proj.bias',\n",
       " 'patch_embed.proj.weight']"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(timm_file.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Swinv2ForImageClassification(\n",
       "  (swinv2): Swinv2Model(\n",
       "    (embeddings): Swinv2Embeddings(\n",
       "      (patch_embeddings): Swinv2PatchEmbeddings(\n",
       "        (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Swinv2Encoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): Swinv2Stage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x Swinv2Layer(\n",
       "              (attention): Swinv2Attention(\n",
       "                (self): Swinv2SelfAttention(\n",
       "                  (continuous_position_bias_mlp): Sequential(\n",
       "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (1): ReLU(inplace=True)\n",
       "                    (2): Linear(in_features=512, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (key): Linear(in_features=128, out_features=128, bias=False)\n",
       "                  (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): Swinv2SelfOutput(\n",
       "                  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): Swinv2DropPath(p=0.1)\n",
       "              (intermediate): Swinv2Intermediate(\n",
       "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (intermediate_act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (output): Swinv2Output(\n",
       "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (downsample): Swinv2PatchMerging(\n",
       "            (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Swinv2Stage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x Swinv2Layer(\n",
       "              (attention): Swinv2Attention(\n",
       "                (self): Swinv2SelfAttention(\n",
       "                  (continuous_position_bias_mlp): Sequential(\n",
       "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (1): ReLU(inplace=True)\n",
       "                    (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (key): Linear(in_features=256, out_features=256, bias=False)\n",
       "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): Swinv2SelfOutput(\n",
       "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): Swinv2DropPath(p=0.1)\n",
       "              (intermediate): Swinv2Intermediate(\n",
       "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (intermediate_act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (output): Swinv2Output(\n",
       "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (downsample): Swinv2PatchMerging(\n",
       "            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Swinv2Stage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-17): 18 x Swinv2Layer(\n",
       "              (attention): Swinv2Attention(\n",
       "                (self): Swinv2SelfAttention(\n",
       "                  (continuous_position_bias_mlp): Sequential(\n",
       "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (1): ReLU(inplace=True)\n",
       "                    (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): Swinv2SelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): Swinv2DropPath(p=0.1)\n",
       "              (intermediate): Swinv2Intermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (output): Swinv2Output(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (downsample): Swinv2PatchMerging(\n",
       "            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): Swinv2Stage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x Swinv2Layer(\n",
       "              (attention): Swinv2Attention(\n",
       "                (self): Swinv2SelfAttention(\n",
       "                  (continuous_position_bias_mlp): Sequential(\n",
       "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (1): ReLU(inplace=True)\n",
       "                    (2): Linear(in_features=512, out_features=32, bias=False)\n",
       "                  )\n",
       "                  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): Swinv2SelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): Swinv2DropPath(p=0.1)\n",
       "              (intermediate): Swinv2Intermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (output): Swinv2Output(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (pooler): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=10861, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Swinv2ForImageClassification(config)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['swinv2.embeddings.patch_embeddings.projection.weight',\n",
       " 'swinv2.embeddings.patch_embeddings.projection.bias',\n",
       " 'swinv2.embeddings.norm.weight',\n",
       " 'swinv2.embeddings.norm.bias',\n",
       " 'swinv2.encoder.layers.0.blocks.0.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.0.blocks.0.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.0.blocks.0.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.0.blocks.0.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.0.blocks.0.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.0.blocks.0.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.0.blocks.0.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.0.blocks.0.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.0.blocks.0.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.0.blocks.0.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.0.blocks.0.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.0.blocks.0.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.0.blocks.0.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.0.blocks.0.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.0.blocks.0.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.0.blocks.0.output.dense.weight',\n",
       " 'swinv2.encoder.layers.0.blocks.0.output.dense.bias',\n",
       " 'swinv2.encoder.layers.0.blocks.0.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.0.blocks.0.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.0.blocks.1.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.0.blocks.1.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.0.blocks.1.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.0.blocks.1.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.0.blocks.1.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.0.blocks.1.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.0.blocks.1.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.0.blocks.1.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.0.blocks.1.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.0.blocks.1.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.0.blocks.1.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.0.blocks.1.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.0.blocks.1.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.0.blocks.1.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.0.blocks.1.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.0.blocks.1.output.dense.weight',\n",
       " 'swinv2.encoder.layers.0.blocks.1.output.dense.bias',\n",
       " 'swinv2.encoder.layers.0.blocks.1.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.0.blocks.1.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.0.downsample.reduction.weight',\n",
       " 'swinv2.encoder.layers.0.downsample.norm.weight',\n",
       " 'swinv2.encoder.layers.0.downsample.norm.bias',\n",
       " 'swinv2.encoder.layers.1.blocks.0.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.1.blocks.0.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.1.blocks.0.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.1.blocks.0.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.1.blocks.0.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.1.blocks.0.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.1.blocks.0.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.1.blocks.0.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.1.blocks.0.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.1.blocks.0.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.1.blocks.0.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.1.blocks.0.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.1.blocks.0.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.1.blocks.0.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.1.blocks.0.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.1.blocks.0.output.dense.weight',\n",
       " 'swinv2.encoder.layers.1.blocks.0.output.dense.bias',\n",
       " 'swinv2.encoder.layers.1.blocks.0.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.1.blocks.0.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.1.blocks.1.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.1.blocks.1.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.1.blocks.1.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.1.blocks.1.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.1.blocks.1.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.1.blocks.1.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.1.blocks.1.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.1.blocks.1.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.1.blocks.1.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.1.blocks.1.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.1.blocks.1.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.1.blocks.1.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.1.blocks.1.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.1.blocks.1.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.1.blocks.1.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.1.blocks.1.output.dense.weight',\n",
       " 'swinv2.encoder.layers.1.blocks.1.output.dense.bias',\n",
       " 'swinv2.encoder.layers.1.blocks.1.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.1.blocks.1.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.1.downsample.reduction.weight',\n",
       " 'swinv2.encoder.layers.1.downsample.norm.weight',\n",
       " 'swinv2.encoder.layers.1.downsample.norm.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.0.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.2.blocks.0.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.0.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.0.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.0.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.0.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.0.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.0.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.0.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.0.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.0.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.0.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.0.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.0.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.0.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.0.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.0.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.0.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.0.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.1.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.2.blocks.1.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.1.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.1.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.1.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.1.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.1.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.1.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.1.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.1.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.1.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.1.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.1.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.1.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.1.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.1.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.1.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.1.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.1.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.2.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.2.blocks.2.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.2.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.2.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.2.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.2.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.2.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.2.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.2.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.2.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.2.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.2.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.2.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.2.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.2.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.2.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.2.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.2.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.2.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.3.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.2.blocks.3.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.3.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.3.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.3.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.3.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.3.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.3.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.3.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.3.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.3.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.3.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.3.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.3.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.3.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.3.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.3.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.3.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.3.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.4.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.2.blocks.4.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.4.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.4.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.4.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.4.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.4.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.4.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.4.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.4.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.4.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.4.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.4.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.4.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.4.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.4.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.4.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.4.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.4.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.5.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.2.blocks.5.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.5.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.5.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.5.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.5.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.5.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.5.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.5.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.5.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.5.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.5.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.5.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.5.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.5.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.5.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.5.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.5.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.5.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.6.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.2.blocks.6.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.6.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.6.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.6.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.6.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.6.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.6.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.6.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.6.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.6.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.6.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.6.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.6.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.6.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.6.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.6.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.6.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.6.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.7.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.2.blocks.7.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.7.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.7.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.7.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.7.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.7.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.7.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.7.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.7.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.7.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.7.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.7.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.7.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.7.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.7.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.7.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.7.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.7.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.8.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.2.blocks.8.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.8.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.8.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.8.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.8.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.8.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.8.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.8.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.8.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.8.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.8.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.8.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.8.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.8.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.8.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.8.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.8.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.8.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.9.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.2.blocks.9.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.9.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.9.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.9.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.9.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.9.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.9.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.9.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.9.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.9.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.9.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.9.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.9.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.9.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.9.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.9.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.9.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.9.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.10.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.2.blocks.10.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.10.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.10.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.10.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.10.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.10.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.10.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.10.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.10.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.10.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.10.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.10.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.10.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.10.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.10.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.10.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.10.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.10.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.11.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.2.blocks.11.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.11.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.11.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.11.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.11.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.11.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.11.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.11.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.11.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.11.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.11.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.11.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.11.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.11.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.11.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.11.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.11.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.11.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.12.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.2.blocks.12.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.12.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.12.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.12.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.12.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.12.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.12.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.12.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.12.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.12.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.12.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.12.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.12.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.12.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.12.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.12.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.12.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.12.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.13.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.2.blocks.13.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.13.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.13.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.13.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.13.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.13.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.13.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.13.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.13.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.13.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.13.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.13.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.13.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.13.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.13.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.13.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.13.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.13.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.14.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.2.blocks.14.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.14.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.14.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.14.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.14.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.14.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.14.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.14.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.14.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.14.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.14.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.14.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.14.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.14.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.14.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.14.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.14.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.14.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.15.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.2.blocks.15.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.15.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.15.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.15.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.15.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.15.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.15.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.15.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.15.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.15.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.15.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.15.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.15.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.15.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.15.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.15.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.15.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.15.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.16.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.2.blocks.16.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.16.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.16.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.16.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.16.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.16.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.16.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.16.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.16.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.16.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.16.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.16.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.16.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.16.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.16.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.16.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.16.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.16.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.17.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.2.blocks.17.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.17.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.17.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.17.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.17.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.17.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.17.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.17.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.17.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.17.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.17.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.17.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.17.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.17.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.17.output.dense.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.17.output.dense.bias',\n",
       " 'swinv2.encoder.layers.2.blocks.17.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.2.blocks.17.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.2.downsample.reduction.weight',\n",
       " 'swinv2.encoder.layers.2.downsample.norm.weight',\n",
       " 'swinv2.encoder.layers.2.downsample.norm.bias',\n",
       " 'swinv2.encoder.layers.3.blocks.0.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.3.blocks.0.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.3.blocks.0.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.3.blocks.0.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.3.blocks.0.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.3.blocks.0.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.3.blocks.0.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.3.blocks.0.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.3.blocks.0.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.3.blocks.0.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.3.blocks.0.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.3.blocks.0.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.3.blocks.0.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.3.blocks.0.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.3.blocks.0.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.3.blocks.0.output.dense.weight',\n",
       " 'swinv2.encoder.layers.3.blocks.0.output.dense.bias',\n",
       " 'swinv2.encoder.layers.3.blocks.0.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.3.blocks.0.layernorm_after.bias',\n",
       " 'swinv2.encoder.layers.3.blocks.1.attention.self.logit_scale',\n",
       " 'swinv2.encoder.layers.3.blocks.1.attention.self.continuous_position_bias_mlp.0.weight',\n",
       " 'swinv2.encoder.layers.3.blocks.1.attention.self.continuous_position_bias_mlp.0.bias',\n",
       " 'swinv2.encoder.layers.3.blocks.1.attention.self.continuous_position_bias_mlp.2.weight',\n",
       " 'swinv2.encoder.layers.3.blocks.1.attention.self.query.weight',\n",
       " 'swinv2.encoder.layers.3.blocks.1.attention.self.query.bias',\n",
       " 'swinv2.encoder.layers.3.blocks.1.attention.self.key.weight',\n",
       " 'swinv2.encoder.layers.3.blocks.1.attention.self.value.weight',\n",
       " 'swinv2.encoder.layers.3.blocks.1.attention.self.value.bias',\n",
       " 'swinv2.encoder.layers.3.blocks.1.attention.output.dense.weight',\n",
       " 'swinv2.encoder.layers.3.blocks.1.attention.output.dense.bias',\n",
       " 'swinv2.encoder.layers.3.blocks.1.layernorm_before.weight',\n",
       " 'swinv2.encoder.layers.3.blocks.1.layernorm_before.bias',\n",
       " 'swinv2.encoder.layers.3.blocks.1.intermediate.dense.weight',\n",
       " 'swinv2.encoder.layers.3.blocks.1.intermediate.dense.bias',\n",
       " 'swinv2.encoder.layers.3.blocks.1.output.dense.weight',\n",
       " 'swinv2.encoder.layers.3.blocks.1.output.dense.bias',\n",
       " 'swinv2.encoder.layers.3.blocks.1.layernorm_after.weight',\n",
       " 'swinv2.encoder.layers.3.blocks.1.layernorm_after.bias',\n",
       " 'swinv2.layernorm.weight',\n",
       " 'swinv2.layernorm.bias',\n",
       " 'classifier.weight',\n",
       " 'classifier.bias']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(model.state_dict().keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_state_dict = convert_state_dict(timm_file, model)\n",
    "model.load_state_dict(new_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([8.9945e-01, 9.3239e-02, 1.1332e-03,  ..., 1.8489e-06, 5.5417e-07,\n",
       "        1.2250e-06])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_processor = AutoImageProcessor.from_pretrained(\"swinv2-v3-config\")\n",
    "image = Image.open(\"./sample.jpg\")\n",
    "inputs = image_processor(images=image, return_tensors=\"pt\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**inputs)\n",
    "\n",
    "logits = torch.sigmoid(outputs.logits[0])\n",
    "logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1girl': tensor(0.9938),\n",
       " 'solo': tensor(0.9601),\n",
       " 'school_uniform': tensor(0.9362),\n",
       " 'skirt': tensor(0.9341),\n",
       " 'short_hair': tensor(0.9256),\n",
       " 'outdoors': tensor(0.9167),\n",
       " 'rating:general': tensor(0.8994),\n",
       " 'serafuku': tensor(0.8894),\n",
       " 'cloud': tensor(0.8597),\n",
       " 'sky': tensor(0.8377),\n",
       " 'sailor_collar': tensor(0.7660),\n",
       " 'black_skirt': tensor(0.7299),\n",
       " 'pleated_skirt': tensor(0.7179),\n",
       " 'shirt': tensor(0.6580),\n",
       " 'long_sleeves': tensor(0.6228),\n",
       " 'water': tensor(0.5572),\n",
       " 'neckerchief': tensor(0.5487),\n",
       " 'black_hair': tensor(0.5393),\n",
       " 'cowboy_shot': tensor(0.5113),\n",
       " 'sunset': tensor(0.5108),\n",
       " 'from_behind': tensor(0.5089),\n",
       " 'black_sailor_collar': tensor(0.5017),\n",
       " 'cloudy_sky': tensor(0.4963),\n",
       " 'black_serafuku': tensor(0.4919),\n",
       " 'ocean': tensor(0.4827),\n",
       " 'scenery': tensor(0.4747),\n",
       " 'black_shirt': tensor(0.4547),\n",
       " 'standing': tensor(0.4287),\n",
       " 'profile': tensor(0.4270),\n",
       " 'lighthouse': tensor(0.4247),\n",
       " 'brown_hair': tensor(0.4151),\n",
       " 'bottle': tensor(0.3790),\n",
       " 'horizon': tensor(0.3716),\n",
       " 'railing': tensor(0.3111),\n",
       " 'closed_mouth': tensor(0.3103),\n",
       " 'brown_skirt': tensor(0.3071),\n",
       " 'floating_hair': tensor(0.2735),\n",
       " 'bob_cut': tensor(0.2693),\n",
       " 'wind': tensor(0.2670),\n",
       " 'boat': tensor(0.2265),\n",
       " 'watercraft': tensor(0.2134),\n",
       " 'looking_back': tensor(0.2013),\n",
       " 'blue_sky': tensor(0.1895),\n",
       " 'mountainous_horizon': tensor(0.1830),\n",
       " 'orange_sky': tensor(0.1770),\n",
       " 'looking_to_the_side': tensor(0.1696),\n",
       " 'black_eyes': tensor(0.1613),\n",
       " 'evening': tensor(0.1530),\n",
       " 'day': tensor(0.1424),\n",
       " 'building': tensor(0.1419),\n",
       " 'bucket': tensor(0.1405),\n",
       " 'looking_afar': tensor(0.1403),\n",
       " 'expressionless': tensor(0.1190),\n",
       " 'tower': tensor(0.1169),\n",
       " 'brown_shirt': tensor(0.1075),\n",
       " 'mountain': tensor(0.1058),\n",
       " 'brown_eyes': tensor(0.1041),\n",
       " 'twilight': tensor(0.1020),\n",
       " 'rating:sensitive': tensor(0.0932),\n",
       " 'bridge': tensor(0.0894),\n",
       " 'lake': tensor(0.0792),\n",
       " 'river': tensor(0.0734),\n",
       " 'miniskirt': tensor(0.0728),\n",
       " 'pier': tensor(0.0727),\n",
       " 'looking_ahead': tensor(0.0710),\n",
       " 'yellow_sky': tensor(0.0690),\n",
       " 'fence': tensor(0.0685),\n",
       " 'town': tensor(0.0623),\n",
       " 'blue_neckerchief': tensor(0.0597),\n",
       " 'hill': tensor(0.0591),\n",
       " 'dusk': tensor(0.0540),\n",
       " 'from_side': tensor(0.0510),\n",
       " 'parted_lips': tensor(0.0490),\n",
       " 'colored_skin': tensor(0.0487),\n",
       " 'waves': tensor(0.0439),\n",
       " 'brown_sailor_collar': tensor(0.0433),\n",
       " 'beach': tensor(0.0408),\n",
       " 'sailor_collar_lift': tensor(0.0387),\n",
       " 'reflection': tensor(0.0387),\n",
       " 'brown_serafuku': tensor(0.0384),\n",
       " 'purple_neckerchief': tensor(0.0381),\n",
       " 'red_neckerchief': tensor(0.0374),\n",
       " 'cityscape': tensor(0.0373),\n",
       " 'medium_hair': tensor(0.0363),\n",
       " 'necktie': tensor(0.0357),\n",
       " 'signature': tensor(0.0325),\n",
       " 'smile': tensor(0.0324),\n",
       " 'hair_between_eyes': tensor(0.0323),\n",
       " 'utility_pole': tensor(0.0318),\n",
       " 'shore': tensor(0.0305),\n",
       " 'uniform': tensor(0.0300),\n",
       " 'sleeve_cuffs': tensor(0.0281),\n",
       " 'house': tensor(0.0276),\n",
       " 'shadow': tensor(0.0276),\n",
       " 'landscape': tensor(0.0266),\n",
       " 'sailor_shirt': tensor(0.0262),\n",
       " 'wind_lift': tensor(0.0255),\n",
       " 'feet_out_of_frame': tensor(0.0250),\n",
       " 'dock': tensor(0.0249),\n",
       " 'gradient_sky': tensor(0.0245),\n",
       " 'black_neckerchief': tensor(0.0245),\n",
       " 'wide_shot': tensor(0.0243),\n",
       " 'traditional_media': tensor(0.0241),\n",
       " 'blouse': tensor(0.0229),\n",
       " 'ship': tensor(0.0228),\n",
       " 'looking_at_viewer': tensor(0.0225),\n",
       " 'pale_skin': tensor(0.0224),\n",
       " 'water_bottle': tensor(0.0222),\n",
       " 'holding': tensor(0.0217),\n",
       " 'sunlight': tensor(0.0214),\n",
       " 'reflective_water': tensor(0.0212),\n",
       " 'balcony': tensor(0.0205),\n",
       " 'turning_head': tensor(0.0204),\n",
       " 'city': tensor(0.0204),\n",
       " 'jar': tensor(0.0193),\n",
       " 'against_railing': tensor(0.0191),\n",
       " 'walking': tensor(0.0190),\n",
       " 'crane_(machine)': tensor(0.0187),\n",
       " 'red_skirt': tensor(0.0187),\n",
       " 'thighhighs': tensor(0.0179),\n",
       " 'blush': tensor(0.0177),\n",
       " 'grey_sky': tensor(0.0176),\n",
       " 'facing_away': tensor(0.0168),\n",
       " 'clock': tensor(0.0166),\n",
       " 'bag': tensor(0.0164),\n",
       " 'artist_name': tensor(0.0162),\n",
       " 'back': tensor(0.0158),\n",
       " 'island': tensor(0.0151),\n",
       " 'legs_together': tensor(0.0149),\n",
       " 'film_grain': tensor(0.0145),\n",
       " 'white_shirt': tensor(0.0144),\n",
       " 'white_sailor_collar': tensor(0.0141),\n",
       " 'sidelocks': tensor(0.0138),\n",
       " 'blurry': tensor(0.0138),\n",
       " 'rooftop': tensor(0.0133),\n",
       " 'tree': tensor(0.0127),\n",
       " 'bird': tensor(0.0126),\n",
       " 'medium_skirt': tensor(0.0121),\n",
       " 'open_mouth': tensor(0.0120),\n",
       " 'long_hair': tensor(0.0120),\n",
       " 'cup': tensor(0.0120),\n",
       " 'orange_theme': tensor(0.0118),\n",
       " 'power_lines': tensor(0.0117),\n",
       " 'sunrise': tensor(0.0116),\n",
       " 'sun': tensor(0.0116),\n",
       " 'outstretched_arms': tensor(0.0114),\n",
       " 'closed_eyes': tensor(0.0112),\n",
       " 'backlighting': tensor(0.0108),\n",
       " 'guard_rail': tensor(0.0108),\n",
       " 'floating_clothes': tensor(0.0106),\n",
       " 'limited_palette': tensor(0.0106),\n",
       " 'glass_bottle': tensor(0.0105),\n",
       " 'blue_eyes': tensor(0.0099),\n",
       " 'thighs': tensor(0.0098),\n",
       " 'road': tensor(0.0097),\n",
       " 'breasts': tensor(0.0097),\n",
       " 'red_shirt': tensor(0.0097),\n",
       " 'blue_sailor_collar': tensor(0.0095),\n",
       " 'skyline': tensor(0.0095),\n",
       " 'dated': tensor(0.0093),\n",
       " 'grey_eyes': tensor(0.0092),\n",
       " 'twitter_username': tensor(0.0091),\n",
       " 'wooden_fence': tensor(0.0088),\n",
       " 'puffy_long_sleeves': tensor(0.0088),\n",
       " 'dress': tensor(0.0087),\n",
       " 'hair_behind_ear': tensor(0.0087),\n",
       " 'collared_shirt': tensor(0.0086),\n",
       " 'cumulonimbus_cloud': tensor(0.0084),\n",
       " 'white_skin': tensor(0.0081),\n",
       " 'arms_at_sides': tensor(0.0079),\n",
       " 'light_smile': tensor(0.0079),\n",
       " 'drum_(container)': tensor(0.0078),\n",
       " 'album_cover': tensor(0.0078),\n",
       " 'looking_up': tensor(0.0077),\n",
       " 'blunt_bangs': tensor(0.0077),\n",
       " 'trash_can': tensor(0.0077),\n",
       " 'overcast': tensor(0.0076),\n",
       " 'watermark': tensor(0.0076),\n",
       " 'faux_traditional_media': tensor(0.0076),\n",
       " 'stairs': tensor(0.0076),\n",
       " 'cliff': tensor(0.0076),\n",
       " 'sand': tensor(0.0075),\n",
       " 'red_sky': tensor(0.0075),\n",
       " 'leaning_on_object': tensor(0.0074),\n",
       " 'long_bangs': tensor(0.0073),\n",
       " 'arm_support': tensor(0.0073),\n",
       " 'blurry_background': tensor(0.0071),\n",
       " 'morning': tensor(0.0069),\n",
       " 'puffy_sleeves': tensor(0.0069),\n",
       " 'lamppost': tensor(0.0069),\n",
       " 'sideways_glance': tensor(0.0068),\n",
       " 'wading': tensor(0.0066),\n",
       " 'on_roof': tensor(0.0065),\n",
       " 'chimney': tensor(0.0065),\n",
       " 'castle': tensor(0.0064),\n",
       " 'half-closed_eyes': tensor(0.0063),\n",
       " 'summer': tensor(0.0063),\n",
       " 'dawn': tensor(0.0062),\n",
       " 'grass': tensor(0.0062),\n",
       " 'surreal': tensor(0.0061),\n",
       " 'silhouette': tensor(0.0061),\n",
       " 'facing_to_the_side': tensor(0.0060),\n",
       " 'dark_clouds': tensor(0.0060),\n",
       " 'acrylic_paint_(medium)': tensor(0.0060),\n",
       " 'ribbon': tensor(0.0059),\n",
       " 'brown_dress': tensor(0.0058),\n",
       " 'painting_(medium)': tensor(0.0058),\n",
       " 'table': tensor(0.0058),\n",
       " 'railroad_crossing': tensor(0.0057),\n",
       " 'black_dress': tensor(0.0056),\n",
       " 'cover': tensor(0.0056),\n",
       " 'floating': tensor(0.0055),\n",
       " 'light_blush': tensor(0.0054),\n",
       " 'on_railing': tensor(0.0054),\n",
       " 'hair_ornament': tensor(0.0054),\n",
       " 'vase': tensor(0.0052),\n",
       " 'muted_color': tensor(0.0050),\n",
       " 'bow': tensor(0.0050),\n",
       " 'white_thighhighs': tensor(0.0048),\n",
       " 'sailor_dress': tensor(0.0047),\n",
       " 'pantyhose': tensor(0.0047),\n",
       " 'eyelashes': tensor(0.0047),\n",
       " 'plant': tensor(0.0047),\n",
       " 'basket': tensor(0.0047),\n",
       " 'black_thighhighs': tensor(0.0046),\n",
       " 'balancing': tensor(0.0045),\n",
       " 'barrel': tensor(0.0045),\n",
       " 'rock': tensor(0.0045),\n",
       " 'red_eyes': tensor(0.0044),\n",
       " 'blue_skin': tensor(0.0043),\n",
       " 'depth_of_field': tensor(0.0042),\n",
       " 'flower': tensor(0.0041),\n",
       " 'ladder': tensor(0.0041),\n",
       " 'ascot': tensor(0.0041),\n",
       " 'purple_eyes': tensor(0.0041),\n",
       " 'holding_bottle': tensor(0.0040),\n",
       " 'can': tensor(0.0040),\n",
       " 'light_frown': tensor(0.0040),\n",
       " 'fantasy': tensor(0.0039),\n",
       " 'pleated_dress': tensor(0.0039),\n",
       " 'socks': tensor(0.0039),\n",
       " 'pink_sky': tensor(0.0038),\n",
       " 'blue_skirt': tensor(0.0038),\n",
       " 'skirt_set': tensor(0.0038),\n",
       " 'running': tensor(0.0037),\n",
       " 'outstretched_arm': tensor(0.0037),\n",
       " 'light_particles': tensor(0.0037),\n",
       " 'loose_hair_strand': tensor(0.0037),\n",
       " 'shirt_tucked_in': tensor(0.0037),\n",
       " 'hand_up': tensor(0.0036),\n",
       " 'clock_tower': tensor(0.0036),\n",
       " 'hands_up': tensor(0.0035),\n",
       " 'orange_background': tensor(0.0035),\n",
       " 'wine_bottle': tensor(0.0035),\n",
       " 'container': tensor(0.0035),\n",
       " ':|': tensor(0.0035),\n",
       " 'brown_neckerchief': tensor(0.0035),\n",
       " 'frown': tensor(0.0035),\n",
       " 'blunt_ends': tensor(0.0034),\n",
       " 'blue_necktie': tensor(0.0034),\n",
       " 'no_pupils': tensor(0.0034),\n",
       " 'ass': tensor(0.0033),\n",
       " 'zettai_ryouiki': tensor(0.0033),\n",
       " 'bare_legs': tensor(0.0032),\n",
       " 'skirt_lift': tensor(0.0032),\n",
       " 'swept_bangs': tensor(0.0031),\n",
       " 'high-waist_skirt': tensor(0.0031),\n",
       " 'school_bag': tensor(0.0031),\n",
       " 'hairclip': tensor(0.0031),\n",
       " 'sideways_mouth': tensor(0.0030),\n",
       " 'medium_breasts': tensor(0.0030),\n",
       " 'border': tensor(0.0030),\n",
       " 'white_neckerchief': tensor(0.0030),\n",
       " 'choppy_bangs': tensor(0.0029),\n",
       " 'ripples': tensor(0.0029),\n",
       " 'leaning_back': tensor(0.0029),\n",
       " 'sign': tensor(0.0029),\n",
       " 'green_eyes': tensor(0.0028),\n",
       " 'blue_serafuku': tensor(0.0028),\n",
       " 'upper_body': tensor(0.0027),\n",
       " 'window': tensor(0.0027),\n",
       " 'monochrome': tensor(0.0025),\n",
       " 'white_pantyhose': tensor(0.0025),\n",
       " 'yellow_eyes': tensor(0.0025),\n",
       " 'leaning_forward': tensor(0.0025),\n",
       " 'sleeves_past_wrists': tensor(0.0025),\n",
       " 'windmill': tensor(0.0025),\n",
       " 'purple_skirt': tensor(0.0024),\n",
       " 'chair': tensor(0.0024),\n",
       " 'lantern': tensor(0.0024),\n",
       " 'nature': tensor(0.0024),\n",
       " 'looking_down': tensor(0.0024),\n",
       " 'purple_hair': tensor(0.0024),\n",
       " 'floating_object': tensor(0.0024),\n",
       " 'painterly': tensor(0.0024),\n",
       " 'jacket': tensor(0.0024),\n",
       " 'rice_paddy': tensor(0.0023),\n",
       " 'alcohol': tensor(0.0023),\n",
       " 'small_breasts': tensor(0.0023),\n",
       " 'short_sleeves': tensor(0.0023),\n",
       " 'transparent': tensor(0.0022),\n",
       " 'railroad_tracks': tensor(0.0022),\n",
       " 'single_stripe': tensor(0.0022),\n",
       " 'looking_at_another': tensor(0.0022),\n",
       " 'tears': tensor(0.0022),\n",
       " 'grey_hair': tensor(0.0022),\n",
       " 'red_sailor_collar': tensor(0.0022),\n",
       " 'sailor': tensor(0.0022),\n",
       " 'sake_bottle': tensor(0.0021),\n",
       " 'white_sky': tensor(0.0021),\n",
       " 'simple_background': tensor(0.0021),\n",
       " 'mole': tensor(0.0021),\n",
       " 'bench': tensor(0.0021),\n",
       " 'grey_neckerchief': tensor(0.0021),\n",
       " 'messy_hair': tensor(0.0021),\n",
       " 'grey_shirt': tensor(0.0021),\n",
       " 'sitting': tensor(0.0021),\n",
       " 'blue_hair': tensor(0.0020),\n",
       " 'sketch': tensor(0.0020),\n",
       " 'see-through_silhouette': tensor(0.0020),\n",
       " 'pointy_ears': tensor(0.0020),\n",
       " 'box': tensor(0.0020),\n",
       " 'thigh_gap': tensor(0.0020),\n",
       " 'wooden_bucket': tensor(0.0020),\n",
       " 'nape': tensor(0.0020),\n",
       " 'arm_at_side': tensor(0.0020),\n",
       " 'spread_arms': tensor(0.0020),\n",
       " 'fishing_rod': tensor(0.0019),\n",
       " 'clothes_lift': tensor(0.0019),\n",
       " 'path': tensor(0.0019),\n",
       " 'mole_under_eye': tensor(0.0019),\n",
       " 'flying': tensor(0.0019),\n",
       " 'teapot': tensor(0.0019),\n",
       " 'oar': tensor(0.0019),\n",
       " 'cover_image': tensor(0.0019),\n",
       " 'purple_shirt': tensor(0.0019),\n",
       " 'drink': tensor(0.0019),\n",
       " 'brown_theme': tensor(0.0019),\n",
       " 'orange_eyes': tensor(0.0019),\n",
       " '1boy': tensor(0.0018),\n",
       " 'animal': tensor(0.0018),\n",
       " 'own_hands_together': tensor(0.0018),\n",
       " 'lifebuoy': tensor(0.0018),\n",
       " 'chromatic_aberration': tensor(0.0018),\n",
       " 'fingernails': tensor(0.0018),\n",
       " '1other': tensor(0.0018),\n",
       " 'radio_antenna': tensor(0.0018),\n",
       " 'hat': tensor(0.0018),\n",
       " 'architecture': tensor(0.0018),\n",
       " 'raised_eyebrows': tensor(0.0018),\n",
       " 'tareme': tensor(0.0017),\n",
       " 'blue_theme': tensor(0.0017),\n",
       " 'jewelry': tensor(0.0017),\n",
       " 'facing_viewer': tensor(0.0017),\n",
       " 'blonde_hair': tensor(0.0017),\n",
       " 'bowl': tensor(0.0017),\n",
       " 'long_skirt': tensor(0.0017),\n",
       " 'virtual_youtuber': tensor(0.0017),\n",
       " 'white_stripes': tensor(0.0017),\n",
       " 'cross': tensor(0.0017),\n",
       " 'book': tensor(0.0017),\n",
       " 'mount_fuji': tensor(0.0017),\n",
       " 'empty_eyes': tensor(0.0017),\n",
       " 'real_world_location': tensor(0.0017),\n",
       " 'yellow_background': tensor(0.0017),\n",
       " 'full_body': tensor(0.0016),\n",
       " 'partially_submerged': tensor(0.0016),\n",
       " 'fog': tensor(0.0016),\n",
       " 'indoors': tensor(0.0016),\n",
       " 'village': tensor(0.0016),\n",
       " 'purple_sailor_collar': tensor(0.0016),\n",
       " 'moon': tensor(0.0015),\n",
       " 'seagull': tensor(0.0015),\n",
       " 'watercolor_(medium)': tensor(0.0015),\n",
       " 'caustics': tensor(0.0015),\n",
       " 'teeth': tensor(0.0015),\n",
       " 'splashing': tensor(0.0015),\n",
       " 'photo_background': tensor(0.0015),\n",
       " 'dark': tensor(0.0015),\n",
       " 'black_footwear': tensor(0.0015),\n",
       " 'multiple_girls': tensor(0.0015),\n",
       " 'shoes': tensor(0.0014),\n",
       " 'sepia': tensor(0.0014),\n",
       " 'hairband': tensor(0.0014),\n",
       " 'sad': tensor(0.0014),\n",
       " 'rural': tensor(0.0014),\n",
       " 'english_text': tensor(0.0014),\n",
       " 'cactus': tensor(0.0014),\n",
       " 'street': tensor(0.0014),\n",
       " 'blue_shirt': tensor(0.0014),\n",
       " 'red_dress': tensor(0.0014),\n",
       " 'potted_plant': tensor(0.0014),\n",
       " 'hatching_(texture)': tensor(0.0013),\n",
       " 'fine_art_parody': tensor(0.0013),\n",
       " 'eyebrows_hidden_by_hair': tensor(0.0013),\n",
       " 'rust': tensor(0.0013),\n",
       " 'cork': tensor(0.0013),\n",
       " 'smoke': tensor(0.0013),\n",
       " 'wind_turbine': tensor(0.0013),\n",
       " 'city_lights': tensor(0.0013),\n",
       " 'flask': tensor(0.0013),\n",
       " 'artist_logo': tensor(0.0013),\n",
       " 'two-sided_skirt': tensor(0.0013),\n",
       " 'very_short_hair': tensor(0.0013),\n",
       " 'black_socks': tensor(0.0013),\n",
       " 'star_(sky)': tensor(0.0013),\n",
       " 'backpack': tensor(0.0013),\n",
       " 'plaid': tensor(0.0013),\n",
       " 'crying': tensor(0.0013),\n",
       " 'night': tensor(0.0013),\n",
       " 'yellow_theme': tensor(0.0013),\n",
       " 'industrial_pipe': tensor(0.0013),\n",
       " 'ramune': tensor(0.0013),\n",
       " 'light_rays': tensor(0.0013),\n",
       " 'colored_sclera': tensor(0.0013),\n",
       " 'grey_skirt': tensor(0.0012),\n",
       " 'brown_bag': tensor(0.0012),\n",
       " 'collar': tensor(0.0012),\n",
       " 'white_serafuku': tensor(0.0012),\n",
       " 'ligne_claire': tensor(0.0012),\n",
       " 'mirror': tensor(0.0012),\n",
       " 'shade': tensor(0.0012),\n",
       " 'outstretched_hand': tensor(0.0012),\n",
       " 'aircraft': tensor(0.0012),\n",
       " 'androgynous': tensor(0.0012),\n",
       " 'dot_nose': tensor(0.0012),\n",
       " 'winter_uniform': tensor(0.0012),\n",
       " 'black_jacket': tensor(0.0012),\n",
       " 'sidelighting': tensor(0.0012),\n",
       " 'smokestack': tensor(0.0012),\n",
       " 'drinking_glass': tensor(0.0011),\n",
       " 'arm_up': tensor(0.0011),\n",
       " 'glowing': tensor(0.0011),\n",
       " 'multicolored_hair': tensor(0.0011),\n",
       " 'fish': tensor(0.0011),\n",
       " 'rating:questionable': tensor(0.0011),\n",
       " 'arm_rest': tensor(0.0011),\n",
       " 'white_border': tensor(0.0011),\n",
       " 'broom': tensor(0.0011),\n",
       " 'skyscraper': tensor(0.0011),\n",
       " 'large_breasts': tensor(0.0011),\n",
       " 'solo_focus': tensor(0.0011),\n",
       " 'painting_(action)': tensor(0.0011),\n",
       " 'contrail': tensor(0.0011),\n",
       " 'ahoge': tensor(0.0011),\n",
       " 'purple_ribbon': tensor(0.0011),\n",
       " 'frills': tensor(0.0011),\n",
       " 'white_hair': tensor(0.0011),\n",
       " 'bowtie': tensor(0.0011),\n",
       " 'plaid_skirt': tensor(0.0011),\n",
       " 'ghost': tensor(0.0011),\n",
       " 'holding_bucket': tensor(0.0011),\n",
       " 'dot_mouth': tensor(0.0011),\n",
       " 'millipen_(medium)': tensor(0.0011),\n",
       " 'straight_hair': tensor(0.0011),\n",
       " 'no_mouth': tensor(0.0011),\n",
       " 'tire': tensor(0.0011),\n",
       " 'bush': tensor(0.0011),\n",
       " 'white_socks': tensor(0.0011),\n",
       " 'cropped_legs': tensor(0.0011),\n",
       " 'black_necktie': tensor(0.0011),\n",
       " 'white_eyes': tensor(0.0010),\n",
       " 'gloves': tensor(0.0010),\n",
       " 'serious': tensor(0.0010),\n",
       " 'kettle': tensor(0.0010),\n",
       " 'pointing': tensor(0.0010),\n",
       " 'lamp': tensor(0.0010),\n",
       " 'ponytail': tensor(0.0010),\n",
       " 'perfume_bottle': tensor(0.0010),\n",
       " 'earrings': tensor(0.0010),\n",
       " ':o': tensor(0.0010),\n",
       " '2girls': tensor(0.0010),\n",
       " 'purple_serafuku': tensor(0.0010),\n",
       " 'kneehighs': tensor(0.0010),\n",
       " 'tombstone': tensor(0.0010),\n",
       " 'hourglass': tensor(0.0010),\n",
       " 'motor_vehicle': tensor(0.0010),\n",
       " 'cover_page': tensor(0.0010),\n",
       " 'orange_hair': tensor(0.0010),\n",
       " 'red_theme': tensor(0.0010),\n",
       " 'airplane': tensor(0.0010),\n",
       " 'purple_necktie': tensor(0.0010),\n",
       " 'wood': tensor(0.0010),\n",
       " 'car': tensor(0.0010),\n",
       " 'neck_ribbon': tensor(0.0010),\n",
       " 'animal_ears': tensor(0.0010),\n",
       " 'soap_bottle': tensor(0.0010),\n",
       " 'arms_up': tensor(0.0009),\n",
       " 'lens_flare': tensor(0.0009),\n",
       " 'bright_pupils': tensor(0.0009),\n",
       " 'purple_sky': tensor(0.0009),\n",
       " 'lips': tensor(0.0009),\n",
       " 'alternate_costume': tensor(0.0009),\n",
       " 'midriff': tensor(0.0009),\n",
       " 'holding_cup': tensor(0.0009),\n",
       " 'instrument': tensor(0.0009),\n",
       " 'watch': tensor(0.0009),\n",
       " 'frilled_skirt': tensor(0.0009),\n",
       " 'undershirt': tensor(0.0009),\n",
       " 'white_pupils': tensor(0.0009),\n",
       " 'cat': tensor(0.0009),\n",
       " 'weapon': tensor(0.0009),\n",
       " 'gate': tensor(0.0009),\n",
       " 'arms_behind_back': tensor(0.0009),\n",
       " 'belt': tensor(0.0009),\n",
       " 'sleeveless': tensor(0.0009),\n",
       " 'black_pantyhose': tensor(0.0009),\n",
       " 'purple_bow': tensor(0.0009),\n",
       " 'statue': tensor(0.0009),\n",
       " 'leaf': tensor(0.0009),\n",
       " 'train': tensor(0.0009),\n",
       " 'parted_bangs': tensor(0.0009),\n",
       " 'grey_sailor_collar': tensor(0.0009),\n",
       " 'brown_jacket': tensor(0.0009),\n",
       " 'white_background': tensor(0.0009),\n",
       " 'stream': tensor(0.0008),\n",
       " 'fishing': tensor(0.0008),\n",
       " 'midriff_peek': tensor(0.0008),\n",
       " 'bare_shoulders': tensor(0.0008),\n",
       " 'reaching': tensor(0.0008),\n",
       " 'underwear': tensor(0.0008),\n",
       " 'radio': tensor(0.0008),\n",
       " 'fire': tensor(0.0008),\n",
       " 'painting_(object)': tensor(0.0008),\n",
       " 'swim_ring': tensor(0.0008),\n",
       " 'jug_(bottle)': tensor(0.0008),\n",
       " 'retro_artstyle': tensor(0.0008),\n",
       " 'drink_can': tensor(0.0008),\n",
       " 'braid': tensor(0.0008),\n",
       " 'red_hair': tensor(0.0008),\n",
       " 'official_alternate_costume': tensor(0.0008),\n",
       " 'crate': tensor(0.0008),\n",
       " 'in_bucket': tensor(0.0008),\n",
       " 'light_brown_hair': tensor(0.0008),\n",
       " 'military': tensor(0.0008),\n",
       " 'milk': tensor(0.0008),\n",
       " 'glass': tensor(0.0008),\n",
       " 'buttons': tensor(0.0008),\n",
       " 'umbrella': tensor(0.0008),\n",
       " 'graphite_(medium)': tensor(0.0008),\n",
       " 'light': tensor(0.0008),\n",
       " 'grey_serafuku': tensor(0.0008),\n",
       " 'multicolored_clothes': tensor(0.0008),\n",
       " 'orange_shirt': tensor(0.0008),\n",
       " 'copyright_name': tensor(0.0008),\n",
       " 'colored_inner_hair': tensor(0.0008),\n",
       " 'nib_pen_(medium)': tensor(0.0008),\n",
       " 'brown_footwear': tensor(0.0008),\n",
       " 'wide_sleeves': tensor(0.0008),\n",
       " 'tokkuri': tensor(0.0008),\n",
       " 'air_conditioner': tensor(0.0008),\n",
       " 'paper': tensor(0.0008),\n",
       " 'looking_at_object': tensor(0.0008),\n",
       " 'fisheye': tensor(0.0007),\n",
       " 'black_ribbon': tensor(0.0007),\n",
       " 'striped_clothes': tensor(0.0007),\n",
       " 'heart': tensor(0.0007),\n",
       " 'field': tensor(0.0007),\n",
       " 'food': tensor(0.0007),\n",
       " 'pool': tensor(0.0007),\n",
       " 'water_drop': tensor(0.0007),\n",
       " 'camera': tensor(0.0007),\n",
       " 'parody': tensor(0.0007),\n",
       " 'open_hands': tensor(0.0007),\n",
       " 'very_long_hair': tensor(0.0007),\n",
       " 'dress_shirt': tensor(0.0007),\n",
       " 'open_clothes': tensor(0.0007),\n",
       " 'makeup': tensor(0.0007),\n",
       " 'straight-on': tensor(0.0007),\n",
       " 'flower_pot': tensor(0.0007),\n",
       " 'vest': tensor(0.0007),\n",
       " 'open_hand': tensor(0.0007),\n",
       " 'blurry_foreground': tensor(0.0007),\n",
       " 'barefoot': tensor(0.0007),\n",
       " 'gourd': tensor(0.0007),\n",
       " 'abstract_background': tensor(0.0007),\n",
       " 'shorts': tensor(0.0007),\n",
       " 'male_focus': tensor(0.0007),\n",
       " 'carrying': tensor(0.0007),\n",
       " 'blue_bow': tensor(0.0007),\n",
       " 'teacup': tensor(0.0007),\n",
       " 'pink_eyes': tensor(0.0007),\n",
       " 'twintails': tensor(0.0007),\n",
       " 'marker_(medium)': tensor(0.0006),\n",
       " 'leaning': tensor(0.0006),\n",
       " 'phone': tensor(0.0006),\n",
       " 'legs_apart': tensor(0.0006),\n",
       " 'two-sided_fabric': tensor(0.0006),\n",
       " 'tail': tensor(0.0006),\n",
       " 'blue_ribbon': tensor(0.0006),\n",
       " 'sparkle': tensor(0.0006),\n",
       " 'night_sky': tensor(0.0006),\n",
       " 'nose': tensor(0.0006),\n",
       " 'one_eye_closed': tensor(0.0006),\n",
       " 'mixed_media': tensor(0.0006),\n",
       " 'black_sleeves': tensor(0.0006),\n",
       " 'shoulder_bag': tensor(0.0006),\n",
       " 'holding_bag': tensor(0.0006),\n",
       " 'white_skirt': tensor(0.0006),\n",
       " 'hair_over_one_eye': tensor(0.0006),\n",
       " 'hair_ribbon': tensor(0.0006),\n",
       " 'pink_shirt': tensor(0.0006),\n",
       " 'short_dress': tensor(0.0006),\n",
       " 'dark_skin': tensor(0.0006),\n",
       " 'happy': tensor(0.0006),\n",
       " 'ruins': tensor(0.0006),\n",
       " 'jitome': tensor(0.0006),\n",
       " 'vignetting': tensor(0.0006),\n",
       " 'hair_over_eyes': tensor(0.0006),\n",
       " 'road_sign': tensor(0.0006),\n",
       " 'bicycle': tensor(0.0006),\n",
       " 'grey_skin': tensor(0.0006),\n",
       " 'standing_on_one_leg': tensor(0.0006),\n",
       " 'paintbrush': tensor(0.0006),\n",
       " 'military_uniform': tensor(0.0006),\n",
       " 'no_nose': tensor(0.0006),\n",
       " 'ambiguous_gender': tensor(0.0006),\n",
       " 'juliet_sleeves': tensor(0.0006),\n",
       " 'gradient_hair': tensor(0.0006),\n",
       " 'soda_bottle': tensor(0.0006),\n",
       " 'door': tensor(0.0006),\n",
       " 'cigarette': tensor(0.0006),\n",
       " 'ice': tensor(0.0005),\n",
       " 'head_tilt': tensor(0.0005),\n",
       " 'collarbone': tensor(0.0005),\n",
       " 'green_skirt': tensor(0.0005),\n",
       " 'footprints': tensor(0.0005),\n",
       " 'pouring': tensor(0.0005),\n",
       " 'pocket': tensor(0.0005),\n",
       " 'hair_intakes': tensor(0.0005),\n",
       " 'holding_basket': tensor(0.0005),\n",
       " 'thermos': tensor(0.0005),\n",
       " 'glasses': tensor(0.0005),\n",
       " 'star_(symbol)': tensor(0.0005),\n",
       " 'flock': tensor(0.0005),\n",
       " 'head_rest': tensor(0.0005),\n",
       " 'bags_under_eyes': tensor(0.0005),\n",
       " 'desk': tensor(0.0005),\n",
       " 'child': tensor(0.0005),\n",
       " 'halo': tensor(0.0005),\n",
       " 'shoulder_blades': tensor(0.0005),\n",
       " 'fruit': tensor(0.0005),\n",
       " 'candle': tensor(0.0005),\n",
       " 'crying_with_eyes_open': tensor(0.0005),\n",
       " 'branch': tensor(0.0005),\n",
       " 'purple_theme': tensor(0.0005),\n",
       " 'thick_thighs': tensor(0.0005),\n",
       " 'yellow_neckerchief': tensor(0.0005),\n",
       " 'shaded_face': tensor(0.0005),\n",
       " 'chain-link_fence': tensor(0.0005),\n",
       " 'suicide': tensor(0.0005),\n",
       " 'bare_arms': tensor(0.0005),\n",
       " 'clear_sky': tensor(0.0005),\n",
       " 'black_bow': tensor(0.0005),\n",
       " 'stool': tensor(0.0005),\n",
       " 'very_wide_shot': tensor(0.0005),\n",
       " 'briefcase': tensor(0.0005),\n",
       " 'compass': tensor(0.0005),\n",
       " 'cart': tensor(0.0005),\n",
       " 'squatting': tensor(0.0005),\n",
       " 'motion_blur': tensor(0.0005),\n",
       " 'starry_sky': tensor(0.0005),\n",
       " 'pink_hair': tensor(0.0005),\n",
       " 'white_flower': tensor(0.0005),\n",
       " 'school': tensor(0.0005),\n",
       " 'teardrop': tensor(0.0005),\n",
       " 'rope': tensor(0.0005),\n",
       " 'dragonfly': tensor(0.0005),\n",
       " 'no_humans': tensor(0.0005),\n",
       " 'sanpaku': tensor(0.0005),\n",
       " 'turtleneck': tensor(0.0005),\n",
       " 'liquid': tensor(0.0005),\n",
       " 'colored_pencil_(medium)': tensor(0.0005),\n",
       " 'pink_neckerchief': tensor(0.0005),\n",
       " 'school_briefcase': tensor(0.0005),\n",
       " 'green_serafuku': tensor(0.0005),\n",
       " 'crossed_legs': tensor(0.0005),\n",
       " 'drinking_straw': tensor(0.0005),\n",
       " 'vanishing_point': tensor(0.0005),\n",
       " 'church': tensor(0.0005),\n",
       " 'legs': tensor(0.0005),\n",
       " 'rose': tensor(0.0005),\n",
       " 'counter': tensor(0.0005),\n",
       " 'pole': tensor(0.0005),\n",
       " 'borrowed_character': tensor(0.0005),\n",
       " 'depressed': tensor(0.0005),\n",
       " 'disposable_cup': tensor(0.0005),\n",
       " 'forest': tensor(0.0005),\n",
       " 'panties': tensor(0.0005),\n",
       " 'waterfall': tensor(0.0005),\n",
       " 'one-hour_drawing_challenge': tensor(0.0005),\n",
       " 'flipped_hair': tensor(0.0005),\n",
       " 'wings': tensor(0.0005),\n",
       " 'detached_sleeves': tensor(0.0005),\n",
       " 'looking_at_animal': tensor(0.0005),\n",
       " 'coat': tensor(0.0005),\n",
       " 'polka_dot': tensor(0.0005),\n",
       " 'split_mouth': tensor(0.0005),\n",
       " 'clothes_pin': tensor(0.0005),\n",
       " 'scarf': tensor(0.0005),\n",
       " 'towel': tensor(0.0005),\n",
       " 'horns': tensor(0.0005),\n",
       " 'circle_skirt': tensor(0.0005),\n",
       " 'afloat': tensor(0.0005),\n",
       " 'kneepits': tensor(0.0005),\n",
       " 'swing': tensor(0.0005),\n",
       " 'greyscale': tensor(0.0005),\n",
       " 'pants': tensor(0.0005),\n",
       " 'leg_up': tensor(0.0004),\n",
       " 'kitauji_high_school_uniform': tensor(0.0004),\n",
       " 'unworn_headwear': tensor(0.0004),\n",
       " 'capelet': tensor(0.0004),\n",
       " 'single_horizontal_stripe': tensor(0.0004),\n",
       " 'beer_bottle': tensor(0.0004),\n",
       " 'skindentation': tensor(0.0004),\n",
       " 'blue_thighhighs': tensor(0.0004),\n",
       " 'fountain': tensor(0.0004),\n",
       " 'arch': tensor(0.0004),\n",
       " 'sand_castle': tensor(0.0004),\n",
       " 'contrapposto': tensor(0.0004),\n",
       " 'yellow_shirt': tensor(0.0004),\n",
       " 'aqua_eyes': tensor(0.0004),\n",
       " 'hair_bow': tensor(0.0004),\n",
       " 'analog_clock': tensor(0.0004),\n",
       " 'sunbeam': tensor(0.0004),\n",
       " 'from_above': tensor(0.0004),\n",
       " 'web_address': tensor(0.0004),\n",
       " 'median_furrow': tensor(0.0004),\n",
       " 'music': tensor(0.0004),\n",
       " 'see-through': tensor(0.0004),\n",
       " 'laundry': tensor(0.0004),\n",
       " 'cardigan': tensor(0.0004),\n",
       " ':d': tensor(0.0004),\n",
       " 'personification': tensor(0.0004),\n",
       " 'crop_top': tensor(0.0004),\n",
       " 'outside_border': tensor(0.0004),\n",
       " 'vending_machine': tensor(0.0004),\n",
       " 'east_asian_architecture': tensor(0.0004),\n",
       " 'coffee_pot': tensor(0.0004),\n",
       " 'double_horizontal_stripe': tensor(0.0004),\n",
       " 'tan': tensor(0.0004),\n",
       " 'brick_wall': tensor(0.0004),\n",
       " 'black_undershirt': tensor(0.0004),\n",
       " 'aged_down': tensor(0.0004),\n",
       " 'two-tone_hair': tensor(0.0004),\n",
       " 'hair_tie': tensor(0.0004),\n",
       " 'furrowed_brow': tensor(0.0004),\n",
       " 'red_necktie': tensor(0.0004),\n",
       " 'narrowed_eyes': tensor(0.0004),\n",
       " 'skirt_hold': tensor(0.0004),\n",
       " 'gem': tensor(0.0004),\n",
       " 'brown_background': tensor(0.0004),\n",
       " 'striped_skirt': tensor(0.0004),\n",
       " 'two-tone_skirt': tensor(0.0004),\n",
       " 'bicycle_basket': tensor(0.0004),\n",
       " 'torii': tensor(0.0004),\n",
       " 'blush_stickers': tensor(0.0004),\n",
       " 'headband': tensor(0.0004),\n",
       " ':3': tensor(0.0004),\n",
       " 'chest_of_drawers': tensor(0.0004),\n",
       " 'short_necktie': tensor(0.0004),\n",
       " 'swimsuit': tensor(0.0004),\n",
       " 'pitcher_(container)': tensor(0.0004),\n",
       " 'glint': tensor(0.0004),\n",
       " 'bug': tensor(0.0004),\n",
       " 'flat_chest': tensor(0.0004),\n",
       " 'foreshortening': tensor(0.0004),\n",
       " 'one_eye_covered': tensor(0.0004),\n",
       " 'pursed_lips': tensor(0.0004),\n",
       " 'hair_flower': tensor(0.0004),\n",
       " 'green_hair': tensor(0.0004),\n",
       " 'clenched_hand': tensor(0.0004),\n",
       " 'sleeveless_shirt': tensor(0.0004),\n",
       " 'green_neckerchief': tensor(0.0004),\n",
       " 'tongue': tensor(0.0004),\n",
       " 'holding_fishing_rod': tensor(0.0004),\n",
       " 'white_gloves': tensor(0.0004),\n",
       " 'antenna_hair': tensor(0.0004),\n",
       " 'brown_vest': tensor(0.0004),\n",
       " 'w_arms': tensor(0.0004),\n",
       " 'black_shorts': tensor(0.0004),\n",
       " 'sweater': tensor(0.0004),\n",
       " 'whale': tensor(0.0004),\n",
       " 'no_legwear': tensor(0.0004),\n",
       " 'black_nails': tensor(0.0004),\n",
       " 'sweat': tensor(0.0004),\n",
       " 'holding_drink': tensor(0.0004),\n",
       " 'people': tensor(0.0004),\n",
       " 'clothesline': tensor(0.0003),\n",
       " 'yellow_skirt': tensor(0.0003),\n",
       " 'black_ascot': tensor(0.0003),\n",
       " 'plate': tensor(0.0003),\n",
       " 'faucet': tensor(0.0003),\n",
       " 'mug': tensor(0.0003),\n",
       " 'pond': tensor(0.0003),\n",
       " 'machinery': tensor(0.0003),\n",
       " ':/': tensor(0.0003),\n",
       " 'loafers': tensor(0.0003),\n",
       " 'sleeves_rolled_up': tensor(0.0003),\n",
       " 'bubble': tensor(0.0003),\n",
       " 'no_lineart': tensor(0.0003),\n",
       " 'suitcase': tensor(0.0003),\n",
       " 'fringe_trim': tensor(0.0003),\n",
       " 'no_headwear': tensor(0.0003),\n",
       " 'yellow_sclera': tensor(0.0003),\n",
       " 'surprised': tensor(0.0003),\n",
       " 'tube': tensor(0.0003),\n",
       " 'levitation': tensor(0.0003),\n",
       " 'crossed_bangs': tensor(0.0003),\n",
       " 'navel': tensor(0.0003),\n",
       " 'soda_can': tensor(0.0003),\n",
       " 'high_collar': tensor(0.0003),\n",
       " 'dappled_sunlight': tensor(0.0003),\n",
       " 'copyright_notice': tensor(0.0003),\n",
       " 'curtains': tensor(0.0003),\n",
       " 'dithering': tensor(0.0003),\n",
       " 'sake': tensor(0.0003),\n",
       " 'streaked_hair': tensor(0.0003),\n",
       " 'green_sailor_collar': tensor(0.0003),\n",
       " 'pine_tree': tensor(0.0003),\n",
       " 'tearing_up': tensor(0.0003),\n",
       " 'stone_lantern': tensor(0.0003),\n",
       " 'brown_cardigan': tensor(0.0003),\n",
       " 'clenched_hands': tensor(0.0003),\n",
       " 'monster_girl': tensor(0.0003),\n",
       " 'eye_contact': tensor(0.0003),\n",
       " 'twisted_torso': tensor(0.0003),\n",
       " 'dark_blue_hair': tensor(0.0003),\n",
       " 'apartment': tensor(0.0003),\n",
       " 'torn_clothes': tensor(0.0003),\n",
       " 'solid_eyes': tensor(0.0003),\n",
       " 'double-parted_bangs': tensor(0.0003),\n",
       " 'letterboxed': tensor(0.0003),\n",
       " 'log': tensor(0.0003),\n",
       " 'on_ground': tensor(0.0003),\n",
       " 'multiple_monochrome': tensor(0.0003),\n",
       " 'shelf': tensor(0.0003),\n",
       " 'white_necktie': tensor(0.0003),\n",
       " 'choko_(cup)': tensor(0.0003),\n",
       " 'grin': tensor(0.0003),\n",
       " 'scroll': tensor(0.0003),\n",
       " 'elbow_rest': tensor(0.0003),\n",
       " 'hishaku': tensor(0.0003),\n",
       " 'multiple_others': tensor(0.0003),\n",
       " 'short_bangs': tensor(0.0003),\n",
       " 'midair': tensor(0.0003),\n",
       " 'autumn': tensor(0.0003),\n",
       " 'shop': tensor(0.0003),\n",
       " 'blue_bowtie': tensor(0.0003),\n",
       " 'dutch_angle': tensor(0.0003),\n",
       " 'cape': tensor(0.0003),\n",
       " 'black_cardigan': tensor(0.0003),\n",
       " 'full_moon': tensor(0.0003),\n",
       " 'black_cat': tensor(0.0003),\n",
       " 'beer': tensor(0.0003),\n",
       " 'japanese_clothes': tensor(0.0003),\n",
       " 'steam': tensor(0.0003),\n",
       " 'bandages': tensor(0.0003),\n",
       " 'character:tomoe_hotaru': tensor(0.0003),\n",
       " 'airship': tensor(0.0003),\n",
       " 'gakuran': tensor(0.0003),\n",
       " 'purple_bowtie': tensor(0.0003),\n",
       " 'red_bow': tensor(0.0003),\n",
       " 'submerged': tensor(0.0003),\n",
       " 'chess_piece': tensor(0.0003),\n",
       " 'red_headwear': tensor(0.0003),\n",
       " 'upside-down': tensor(0.0003),\n",
       " 'doujin_cover': tensor(0.0003),\n",
       " 'black_border': tensor(0.0003),\n",
       " 'sweatdrop': tensor(0.0003),\n",
       " 'handbag': tensor(0.0003),\n",
       " 'white_footwear': tensor(0.0003),\n",
       " 'wide_hips': tensor(0.0003),\n",
       " 'from_below': tensor(0.0003),\n",
       " 'wooden_floor': tensor(0.0003),\n",
       " 'lotion_bottle': tensor(0.0003),\n",
       " 'pillar': tensor(0.0003),\n",
       " 'purple_skin': tensor(0.0003),\n",
       " 'rain': tensor(0.0003),\n",
       " 'picture_frame': tensor(0.0003),\n",
       " 'thigh_strap': tensor(0.0003),\n",
       " 'winter': tensor(0.0003),\n",
       " 'latin_cross': tensor(0.0003),\n",
       " 'desert': tensor(0.0003),\n",
       " 'drinking': tensor(0.0003),\n",
       " 'choker': tensor(0.0003),\n",
       " '1990s_(style)': tensor(0.0003),\n",
       " 'floating_island': tensor(0.0003),\n",
       " 'playground': tensor(0.0003),\n",
       " 'weibo_logo': tensor(0.0003),\n",
       " 'tattoo': tensor(0.0003),\n",
       " 'red_background': tensor(0.0003),\n",
       " 'weibo_username': tensor(0.0003),\n",
       " 'colorful': tensor(0.0003),\n",
       " 'white_sleeves': tensor(0.0003),\n",
       " 'jumping': tensor(0.0003),\n",
       " 'covered_eyes': tensor(0.0003),\n",
       " 'red_ribbon': tensor(0.0003),\n",
       " 'school_desk': tensor(0.0003),\n",
       " 'snow': tensor(0.0003),\n",
       " 'rating:explicit': tensor(0.0003),\n",
       " 'hair_bun': tensor(0.0003),\n",
       " 'nail_polish': tensor(0.0003),\n",
       " 'high_contrast': tensor(0.0003),\n",
       " 'pink_skirt': tensor(0.0003),\n",
       " 'black_vest': tensor(0.0003),\n",
       " 'cooler': tensor(0.0003),\n",
       " 'wing_collar': tensor(0.0003),\n",
       " 'tiptoes': tensor(0.0003),\n",
       " 'pov': tensor(0.0003),\n",
       " 'lying': tensor(0.0003),\n",
       " 'short_hair_with_long_locks': tensor(0.0003),\n",
       " 'no_sclera': tensor(0.0003),\n",
       " 'cable': tensor(0.0003),\n",
       " 'in_container': tensor(0.0003),\n",
       " 'hand_on_own_hip': tensor(0.0003),\n",
       " 'falling': tensor(0.0003),\n",
       " 'partially_colored': tensor(0.0003),\n",
       " 'knife': tensor(0.0003),\n",
       " 'black_headwear': tensor(0.0003),\n",
       " 'stud_earrings': tensor(0.0003),\n",
       " 'easel': tensor(0.0003),\n",
       " 'white_dress': tensor(0.0003),\n",
       " 'mole_under_mouth': tensor(0.0003),\n",
       " 'innertube': tensor(0.0003),\n",
       " 'map': tensor(0.0003),\n",
       " '2021': tensor(0.0003),\n",
       " 'sleeves_past_elbows': tensor(0.0003),\n",
       " 'cellphone': tensor(0.0003),\n",
       " 'grave': tensor(0.0003),\n",
       " 'dark-skinned_female': tensor(0.0003),\n",
       " 'walking_on_liquid': tensor(0.0002),\n",
       " 'bandaid': tensor(0.0002),\n",
       " 'dancing': tensor(0.0002),\n",
       " 'tentacles': tensor(0.0002),\n",
       " 'halftone': tensor(0.0002),\n",
       " 'sword': tensor(0.0002),\n",
       " 'covered_mouth': tensor(0.0002),\n",
       " 'book_stack': tensor(0.0002),\n",
       " 'formal': tensor(0.0002),\n",
       " 'orange_skirt': tensor(0.0002),\n",
       " 'birdcage': tensor(0.0002),\n",
       " 'cage': tensor(0.0002),\n",
       " 'legs_up': tensor(0.0002),\n",
       " 'floral_print': tensor(0.0002),\n",
       " 'potion': tensor(0.0002),\n",
       " 'bowl_cut': tensor(0.0002),\n",
       " 'crossed_arms': tensor(0.0002),\n",
       " 'brown_shorts': tensor(0.0002),\n",
       " 'nude': tensor(0.0002),\n",
       " 'screentones': tensor(0.0002),\n",
       " 'rigging': tensor(0.0002),\n",
       " 'weighing_scale': tensor(0.0002),\n",
       " 'realistic': tensor(0.0002),\n",
       " 'graveyard': tensor(0.0002),\n",
       " 'round-bottom_flask': tensor(0.0002),\n",
       " 'black_hairband': tensor(0.0002),\n",
       " 'milk_bottle': tensor(0.0002),\n",
       " 'antennae': tensor(0.0002),\n",
       " 'bare_back': tensor(0.0002),\n",
       " 'lace_trim': tensor(0.0002),\n",
       " 'watering_can': tensor(0.0002),\n",
       " 'flag': tensor(0.0002),\n",
       " 'black_bag': tensor(0.0002),\n",
       " 'wristwatch': tensor(0.0002),\n",
       " 'wet': tensor(0.0002),\n",
       " 'crack': tensor(0.0002),\n",
       " 'upper_teeth_only': tensor(0.0002),\n",
       " 'out_of_frame': tensor(0.0002),\n",
       " 'layered_clothes': tensor(0.0002),\n",
       " 'giant': tensor(0.0002),\n",
       " 'boots': tensor(0.0002),\n",
       " 'purple_thighhighs': tensor(0.0002),\n",
       " 'wide-eyed': tensor(0.0002),\n",
       " 'different_reflection': tensor(0.0002),\n",
       " 'orange_sailor_collar': tensor(0.0002),\n",
       " 'crosshatching': tensor(0.0002),\n",
       " '2023': tensor(0.0002),\n",
       " 'tongue_out': tensor(0.0002),\n",
       " 'military_vehicle': tensor(0.0002),\n",
       " 'linear_hatching': tensor(0.0002),\n",
       " 'multicolored_eyes': tensor(0.0002),\n",
       " 'ferris_wheel': tensor(0.0002),\n",
       " 'leaning_to_the_side': tensor(0.0002),\n",
       " 'dripping': tensor(0.0002),\n",
       " 'black_gloves': tensor(0.0002),\n",
       " 'breath': tensor(0.0002),\n",
       " 'plump': tensor(0.0002),\n",
       " 'tag': tensor(0.0002),\n",
       " 'comic': tensor(0.0002),\n",
       " 'back_bow': tensor(0.0002),\n",
       " 'standing_on_liquid': tensor(0.0002),\n",
       " 'blue_flower': tensor(0.0002),\n",
       " 'fishing_line': tensor(0.0002),\n",
       " 'warship': tensor(0.0002),\n",
       " 'official_alternate_hairstyle': tensor(0.0002),\n",
       " 'wooden_table': tensor(0.0002),\n",
       " 'tray': tensor(0.0002),\n",
       " 'spinning': tensor(0.0002),\n",
       " 'cube': tensor(0.0002),\n",
       " ...}"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = {model.config.id2label[i]: logit.float() for i, logit in enumerate(logits)}\n",
    "results = {\n",
    "    k: v for k, v in sorted(results.items(), key=lambda item: item[1], reverse=True)\n",
    "}\n",
    "results  # rating tags and character tags are also included"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Swinv2ForImageClassification(\n",
       "  (swinv2): Swinv2Model(\n",
       "    (embeddings): Swinv2Embeddings(\n",
       "      (patch_embeddings): Swinv2PatchEmbeddings(\n",
       "        (projection): Conv2d(3, 128, kernel_size=(4, 4), stride=(4, 4))\n",
       "      )\n",
       "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): Swinv2Encoder(\n",
       "      (layers): ModuleList(\n",
       "        (0): Swinv2Stage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x Swinv2Layer(\n",
       "              (attention): Swinv2Attention(\n",
       "                (self): Swinv2SelfAttention(\n",
       "                  (continuous_position_bias_mlp): Sequential(\n",
       "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (1): ReLU(inplace=True)\n",
       "                    (2): Linear(in_features=512, out_features=4, bias=False)\n",
       "                  )\n",
       "                  (query): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (key): Linear(in_features=128, out_features=128, bias=False)\n",
       "                  (value): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): Swinv2SelfOutput(\n",
       "                  (dense): Linear(in_features=128, out_features=128, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (layernorm_before): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): Swinv2DropPath(p=0.1)\n",
       "              (intermediate): Swinv2Intermediate(\n",
       "                (dense): Linear(in_features=128, out_features=512, bias=True)\n",
       "                (intermediate_act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (output): Swinv2Output(\n",
       "                (dense): Linear(in_features=512, out_features=128, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (layernorm_after): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (downsample): Swinv2PatchMerging(\n",
       "            (reduction): Linear(in_features=512, out_features=256, bias=False)\n",
       "            (norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (1): Swinv2Stage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x Swinv2Layer(\n",
       "              (attention): Swinv2Attention(\n",
       "                (self): Swinv2SelfAttention(\n",
       "                  (continuous_position_bias_mlp): Sequential(\n",
       "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (1): ReLU(inplace=True)\n",
       "                    (2): Linear(in_features=512, out_features=8, bias=False)\n",
       "                  )\n",
       "                  (query): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (key): Linear(in_features=256, out_features=256, bias=False)\n",
       "                  (value): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): Swinv2SelfOutput(\n",
       "                  (dense): Linear(in_features=256, out_features=256, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (layernorm_before): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): Swinv2DropPath(p=0.1)\n",
       "              (intermediate): Swinv2Intermediate(\n",
       "                (dense): Linear(in_features=256, out_features=1024, bias=True)\n",
       "                (intermediate_act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (output): Swinv2Output(\n",
       "                (dense): Linear(in_features=1024, out_features=256, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (layernorm_after): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (downsample): Swinv2PatchMerging(\n",
       "            (reduction): Linear(in_features=1024, out_features=512, bias=False)\n",
       "            (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (2): Swinv2Stage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-17): 18 x Swinv2Layer(\n",
       "              (attention): Swinv2Attention(\n",
       "                (self): Swinv2SelfAttention(\n",
       "                  (continuous_position_bias_mlp): Sequential(\n",
       "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (1): ReLU(inplace=True)\n",
       "                    (2): Linear(in_features=512, out_features=16, bias=False)\n",
       "                  )\n",
       "                  (query): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (key): Linear(in_features=512, out_features=512, bias=False)\n",
       "                  (value): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): Swinv2SelfOutput(\n",
       "                  (dense): Linear(in_features=512, out_features=512, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (layernorm_before): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): Swinv2DropPath(p=0.1)\n",
       "              (intermediate): Swinv2Intermediate(\n",
       "                (dense): Linear(in_features=512, out_features=2048, bias=True)\n",
       "                (intermediate_act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (output): Swinv2Output(\n",
       "                (dense): Linear(in_features=2048, out_features=512, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (layernorm_after): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "          (downsample): Swinv2PatchMerging(\n",
       "            (reduction): Linear(in_features=2048, out_features=1024, bias=False)\n",
       "            (norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (3): Swinv2Stage(\n",
       "          (blocks): ModuleList(\n",
       "            (0-1): 2 x Swinv2Layer(\n",
       "              (attention): Swinv2Attention(\n",
       "                (self): Swinv2SelfAttention(\n",
       "                  (continuous_position_bias_mlp): Sequential(\n",
       "                    (0): Linear(in_features=2, out_features=512, bias=True)\n",
       "                    (1): ReLU(inplace=True)\n",
       "                    (2): Linear(in_features=512, out_features=32, bias=False)\n",
       "                  )\n",
       "                  (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (key): Linear(in_features=1024, out_features=1024, bias=False)\n",
       "                  (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "                (output): Swinv2SelfOutput(\n",
       "                  (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "                  (dropout): Dropout(p=0.0, inplace=False)\n",
       "                )\n",
       "              )\n",
       "              (layernorm_before): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (drop_path): Swinv2DropPath(p=0.1)\n",
       "              (intermediate): Swinv2Intermediate(\n",
       "                (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "                (intermediate_act_fn): PytorchGELUTanh()\n",
       "              )\n",
       "              (output): Swinv2Output(\n",
       "                (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "                (dropout): Dropout(p=0.0, inplace=False)\n",
       "              )\n",
       "              (layernorm_after): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "    (pooler): AdaptiveAvgPool1d(output_size=1)\n",
       "  )\n",
       "  (classifier): Linear(in_features=1024, out_features=10861, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(torch.bfloat16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(\"swinv2-v3-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.push_to_hub(\"p1atdev/wd-swinv2-tagger-v3-hf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
